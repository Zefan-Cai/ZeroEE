11/06/2023 00:49:16 - INFO - __main__ - ***** Running training *****
11/06/2023 00:49:16 - INFO - __main__ -   Num examples = 151736
11/06/2023 00:49:16 - INFO - __main__ -   Num Epochs = 20
11/06/2023 00:49:16 - INFO - __main__ -   Instantaneous batch size per device = 16
11/06/2023 00:49:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 96
11/06/2023 00:49:16 - INFO - __main__ -   Gradient Accumulation steps = 1
11/06/2023 00:49:16 - INFO - __main__ -   Total optimization steps = 31620
  0%|                                                                                                                                                                         | 0/31620 [00:00<?, ?it/s]11/06/2023 00:49:16 - INFO - accelerate.accelerator - Loading states from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/
11/06/2023 00:49:16 - INFO - accelerate.accelerator - Loading DeepSpeed Model and Optimizer
Resumed from checkpoint: /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/
[2023-11-06 00:49:16,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-11-06 00:49:16,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-11-06 00:49:16,923] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-11-06 00:49:16,934] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-11-06 00:49:16,944] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 00:49:21,275] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 00:49:21,276] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 6 ZeRO state_dicts for rank 0
11/06/2023 00:49:25 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer loaded from input dir /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model
11/06/2023 00:49:25 - INFO - accelerate.checkpointing - All model weights loaded successfully
11/06/2023 00:49:25 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
11/06/2023 00:49:25 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
11/06/2023 00:49:25 - INFO - accelerate.checkpointing - All random states loaded successfully
11/06/2023 00:49:25 - INFO - accelerate.accelerator - Loading in 0 custom states
Traceback (most recent call last):
  File "./open_instruct/open_instruct/finetune_val.py", line 883, in <module>
    main()
  File "./open_instruct/open_instruct/finetune_val.py", line 682, in main
    int(training_difference.replace("step_", ""))
ValueError: invalid literal for int() with base 10: ''
[2023-11-06 00:49:25,296] [INFO] [engine.py:2815:_load_zero_checkpoint] loading 6 zero partition checkpoints for rank 0
debug path