09/11/2023 22:42:23 - INFO - __main__ - ***** Running training *****
09/11/2023 22:42:23 - INFO - __main__ -   Num examples = 5207
09/11/2023 22:42:23 - INFO - __main__ -   Num Epochs = 30
09/11/2023 22:42:23 - INFO - __main__ -   Instantaneous batch size per device = 8
09/11/2023 22:42:23 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
09/11/2023 22:42:23 - INFO - __main__ -   Gradient Accumulation steps = 1
09/11/2023 22:42:23 - INFO - __main__ -   Total optimization steps = 4890

  0%|                                                                                                                                                                    | 1/4890 [00:04<6:43:34,  4.95s/it]09/11/2023 22:42:27 - INFO - __main__ -   Step: 1, LR: 1.3675213675213677e-07, Loss: 7.5259690284729
[2023-09-11 22:42:27,993] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                                    | 2/4890 [00:09<6:20:16,  4.67s/it]09/11/2023 22:42:32 - INFO - __main__ -   Step: 2, LR: 2.7350427350427354e-07, Loss: 7.368340492248535
[2023-09-11 22:42:32,461] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                                    | 3/4890 [00:13<5:56:41,  4.38s/it]09/11/2023 22:42:36 - INFO - __main__ -   Step: 3, LR: 4.102564102564103e-07, Loss: 7.479620933532715
[2023-09-11 22:42:36,497] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▏                                                                                                                                                                   | 4/4890 [00:17<5:45:54,  4.25s/it]09/11/2023 22:42:40 - INFO - __main__ -   Step: 4, LR: 5.470085470085471e-07, Loss: 7.212525367736816
[2023-09-11 22:42:40,543] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▏                                                                                                                                                                   | 5/4890 [00:21<5:46:02,  4.25s/it]09/11/2023 22:42:44 - INFO - __main__ -   Step: 5, LR: 6.837606837606839e-07, Loss: 7.517477035522461
[2023-09-11 22:42:44,798] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▏                                                                                                                                                                   | 6/4890 [00:25<5:43:14,  4.22s/it]09/11/2023 22:42:48 - INFO - __main__ -   Step: 6, LR: 8.205128205128206e-07, Loss: 7.341798782348633
[2023-09-11 22:42:48,949] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▏                                                                                                                                                                   | 7/4890 [00:29<5:34:56,  4.12s/it]09/11/2023 22:42:52 - INFO - __main__ -   Step: 7, LR: 9.572649572649574e-07, Loss: 7.3526082038879395
[2023-09-11 22:42:52,856] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-09-11 22:42:57,489] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▎                                                                                                                                                                   | 8/4890 [00:34<5:48:19,  4.28s/it]09/11/2023 22:42:57 - INFO - __main__ -   Step: 8, LR: 1.0940170940170942e-06, Loss: 7.205732345581055
  0%|▎                                                                                                                                                                   | 9/4890 [00:38<5:40:52,  4.19s/it]09/11/2023 22:43:01 - INFO - __main__ -   Step: 9, LR: 1.230769230769231e-06, Loss: 6.9751667976379395
[2023-09-11 22:43:01,483] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-09-11 22:43:05,863] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▎                                                                                                                                                                  | 10/4890 [00:42<5:45:36,  4.25s/it]09/11/2023 22:43:05 - INFO - __main__ -   Step: 10, LR: 1.3675213675213678e-06, Loss: 6.534173488616943
[2023-09-11 22:43:09,866] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▎                                                                                                                                                                  | 11/4890 [00:46<5:39:23,  4.17s/it]09/11/2023 22:43:09 - INFO - __main__ -   Step: 11, LR: 1.5042735042735044e-06, Loss: 6.298562049865723
[2023-09-11 22:43:13,807] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▍                                                                                                                                                                  | 12/4890 [00:50<5:33:34,  4.10s/it]09/11/2023 22:43:13 - INFO - __main__ -   Step: 12, LR: 1.6410256410256412e-06, Loss: 6.119693756103516
[2023-09-11 22:43:17,876] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▍                                                                                                                                                                  | 13/4890 [00:54<5:32:38,  4.09s/it]09/11/2023 22:43:17 - INFO - __main__ -   Step: 13, LR: 1.777777777777778e-06, Loss: 4.641506195068359
[2023-09-11 22:43:21,774] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▍                                                                                                                                                                  | 14/4890 [00:58<5:27:49,  4.03s/it]09/11/2023 22:43:21 - INFO - __main__ -   Step: 14, LR: 1.9145299145299148e-06, Loss: 4.399111270904541
[2023-09-11 22:43:25,828] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▌                                                                                                                                                                  | 15/4890 [01:02<5:28:13,  4.04s/it]09/11/2023 22:43:25 - INFO - __main__ -   Step: 15, LR: 2.0512820512820513e-06, Loss: 4.057561874389648
[2023-09-11 22:43:29,756] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▌                                                                                                                                                                  | 16/4890 [01:06<5:25:25,  4.01s/it]09/11/2023 22:43:29 - INFO - __main__ -   Step: 16, LR: 2.1880341880341884e-06, Loss: 3.841775417327881
[2023-09-11 22:43:33,802] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▌                                                                                                                                                                  | 17/4890 [01:10<5:26:20,  4.02s/it]09/11/2023 22:43:33 - INFO - __main__ -   Step: 17, LR: 2.324786324786325e-06, Loss: 1.515824317932129
  0%|▌                                                                                                                                                                  | 18/4890 [01:14<5:23:39,  3.99s/it]09/11/2023 22:43:37 - INFO - __main__ -   Step: 18, LR: 2.461538461538462e-06, Loss: 1.0650997161865234
[2023-09-11 22:43:37,713] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-09-11 22:43:41,779] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▋                                                                                                                                                                  | 19/4890 [01:18<5:25:32,  4.01s/it]09/11/2023 22:43:41 - INFO - __main__ -   Step: 19, LR: 2.5982905982905985e-06, Loss: 0.2814632058143616
  0%|▋                                                                                                                                                                  | 20/4890 [01:22<5:23:16,  3.98s/it]09/11/2023 22:43:45 - INFO - __main__ -   Step: 20, LR: 2.7350427350427355e-06, Loss: 0.6392361521720886
[2023-09-11 22:43:45,698] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▋                                                                                                                                                                  | 21/4890 [01:26<5:21:35,  3.96s/it]09/11/2023 22:43:49 - INFO - __main__ -   Step: 21, LR: 2.8717948717948717e-06, Loss: 0.1111101433634758
[2023-09-11 22:43:49,615] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▋                                                                                                                                                                  | 22/4890 [01:30<5:17:51,  3.92s/it]09/11/2023 22:43:53 - INFO - __main__ -   Step: 22, LR: 3.0085470085470087e-06, Loss: 0.20971332490444183
[2023-09-11 22:43:53,428] [WARNING] [stage3.py:1933:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-09-11 22:43:57,766] [WARNING] [stage3.py:1933:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|▊                                                                                                                                                                  | 23/4890 [01:34<5:28:02,  4.04s/it]09/11/2023 22:43:57 - INFO - __main__ -   Step: 23, LR: 3.1452991452991453e-06, Loss: 0.31031930446624756
Traceback (most recent call last):
  File "./open_instruct/open_instruct/finetune.py", line 694, in <module>
    main()
  File "./open_instruct/open_instruct/finetune.py", line 611, in main
    accelerator.backward(loss)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/accelerate/accelerator.py", line 1917, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 176, in backward
    self.engine.step()
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 2116, in step
    self._take_model_step(lr_kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 2022, in _take_model_step
    self.optimizer.step()
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1914, in step
    self._optimizer_step(sub_group_id)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 875, in _optimizer_step
    self.optimizer.step()
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 69, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/adamw.py", line 171, in step
    adamw(
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/adamw.py", line 321, in adamw
    func(
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/optim/adamw.py", line 566, in _multi_tensor_adamw
    denom = torch._foreach_add(exp_avg_sq_sqrt, eps)
KeyboardInterrupt