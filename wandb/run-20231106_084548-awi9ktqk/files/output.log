11/06/2023 08:45:51 - INFO - __main__ - ***** Running training *****
11/06/2023 08:45:51 - INFO - __main__ -   Num examples = 151736
11/06/2023 08:45:51 - INFO - __main__ -   Num Epochs = 20
11/06/2023 08:45:51 - INFO - __main__ -   Instantaneous batch size per device = 16
11/06/2023 08:45:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 96
11/06/2023 08:45:51 - INFO - __main__ -   Gradient Accumulation steps = 1
11/06/2023 08:45:51 - INFO - __main__ -   Total optimization steps = 31620

  0%|                                                                                                                                                              | 1/31620 [00:04<38:36:56,  4.40s/it]11/06/2023 08:45:55 - INFO - __main__ -   Step: 1, LR: 2.1089630931458703e-08, Loss: 5.80912971496582
  0%|                                                                                                                                                              | 2/31620 [00:08<36:26:24,  4.15s/it]11/06/2023 08:45:59 - INFO - __main__ -   Step: 2, LR: 4.2179261862917406e-08, Loss: 5.8994903564453125
  0%|                                                                                                                                                              | 3/31620 [00:12<35:47:35,  4.08s/it]11/06/2023 08:46:03 - INFO - __main__ -   Step: 3, LR: 6.32688927943761e-08, Loss: 5.896520137786865
  0%|                                                                                                                                                              | 4/31620 [00:16<35:32:07,  4.05s/it]11/06/2023 08:46:07 - INFO - __main__ -   Step: 4, LR: 8.435852372583481e-08, Loss: 5.885514259338379
  0%|                                                                                                                                                              | 5/31620 [00:20<35:20:55,  4.03s/it]11/06/2023 08:46:11 - INFO - __main__ -   Step: 5, LR: 1.0544815465729351e-07, Loss: 5.742072105407715
  0%|                                                                                                                                                              | 6/31620 [00:24<35:15:29,  4.01s/it]11/06/2023 08:46:15 - INFO - __main__ -   Step: 6, LR: 1.265377855887522e-07, Loss: 5.937280178070068
  0%|                                                                                                                                                              | 7/31620 [00:28<35:10:37,  4.01s/it]11/06/2023 08:46:19 - INFO - __main__ -   Step: 7, LR: 1.476274165202109e-07, Loss: 5.760401725769043
  0%|                                                                                                                                                              | 8/31620 [00:32<35:08:55,  4.00s/it]11/06/2023 08:46:23 - INFO - __main__ -   Step: 8, LR: 1.6871704745166963e-07, Loss: 5.754495143890381
  0%|                                                                                                                                                              | 9/31620 [00:36<35:06:31,  4.00s/it]11/06/2023 08:46:27 - INFO - __main__ -   Step: 9, LR: 1.898066783831283e-07, Loss: 5.752425193786621
  0%|                                                                                                                                                             | 10/31620 [00:40<35:05:29,  4.00s/it]11/06/2023 08:46:31 - INFO - __main__ -   Step: 10, LR: 2.1089630931458702e-07, Loss: 5.826904296875
11/06/2023 08:46:31 - INFO - __main__ - Start doing evaluation at step: 10









Traceback (most recent call last):█████████████████████████████████████▍                                                                                                | 11/27 [00:19<00:28,  1.75s/it]
  File "./open_instruct/open_instruct/finetune_val.py", line 923, in <module>
    main()
  File "./open_instruct/open_instruct/finetune_val.py", line 834, in main
    outputs = model(**eval_batch, use_cache=False, return_dict = True)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1769, in forward
    loss = self.module(*inputs, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 806, in forward
    outputs = self.model(
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 693, in forward
    layer_outputs = decoder_layer(
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 408, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1538, in _call_impl
    result = forward_call(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py", line 307, in forward
    value_states = self.v_proj(hidden_states)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    result = hook(self, args)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 382, in _pre_forward_module_hook
    self.pre_sub_module_forward_function(module)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 494, in pre_sub_module_forward_function
    param_coordinator.fetch_sub_module(sub_module, forward=True)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 295, in fetch_sub_module
    self.__ongoing_fetch_events.popleft().synchronize()
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/torch/cuda/streams.py", line 219, in synchronize
    super().synchronize()
KeyboardInterrupt