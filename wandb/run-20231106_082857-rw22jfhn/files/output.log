11/06/2023 08:29:00 - INFO - __main__ - ***** Running training *****
11/06/2023 08:29:00 - INFO - __main__ -   Num examples = 151736
11/06/2023 08:29:00 - INFO - __main__ -   Num Epochs = 20
11/06/2023 08:29:00 - INFO - __main__ -   Instantaneous batch size per device = 16
11/06/2023 08:29:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 96
11/06/2023 08:29:00 - INFO - __main__ -   Gradient Accumulation steps = 1
11/06/2023 08:29:00 - INFO - __main__ -   Total optimization steps = 31620
  0%|                                                                                                                                                                         | 0/31620 [00:00<?, ?it/s]11/06/2023 08:29:00 - INFO - accelerate.accelerator - Loading states from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5
11/06/2023 08:29:00 - INFO - accelerate.accelerator - Loading DeepSpeed Model and Optimizer
Resumed from checkpoint: /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5
[2023-11-06 08:29:00,204] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-11-06 08:29:00,219] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-11-06 08:29:00,220] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt...
[2023-11-06 08:29:00,230] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/zero_pp_rank_0_mp_rank_00_model_states.pt.
[2023-11-06 08:29:00,241] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2023-11-06 08:29:04,614] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2023-11-06 08:29:04,615] [INFO] [engine.py:2865:_get_all_zero_checkpoint_state_dicts] successfully read 6 ZeRO state_dicts for rank 0
11/06/2023 08:29:09 - INFO - accelerate.accelerator - DeepSpeed Model and Optimizer loaded from input dir /local1/zefan/output/Llama2_GenData_1definitions_top200/epoch_5/pytorch_model
11/06/2023 08:29:09 - INFO - accelerate.checkpointing - All model weights loaded successfully
11/06/2023 08:29:09 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
11/06/2023 08:29:09 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
11/06/2023 08:29:09 - INFO - accelerate.checkpointing - All random states loaded successfully
11/06/2023 08:29:09 - INFO - accelerate.accelerator - Loading in 0 custom states
 30%|██████████████████████████████████████████████▊                                                                                                             | 9486/31620 [00:09<00:21, 1039.85it/s]

 30%|██████████████████████████████████████████████▊                                                                                                             | 9486/31620 [00:09<00:21, 1039.85it/s]11/06/2023 08:29:13 - INFO - __main__ -   Step: 9487, LR: 1.4431001684874179e-05, Loss: 0.004177939146757126
11/06/2023 08:29:17 - INFO - __main__ -   Step: 9488, LR: 1.4430349475514975e-05, Loss: 0.000276626436971128
11/06/2023 08:29:18 - INFO - __main__ - Start doing evaluation at step: 9488
  0%|                                                                                                                                                                             | 0/7 [00:00<?, ?it/s]
debug batch_predict 128 [29889, 29889, 29889, 29889, 29889, 29889, 29889, 29889, 29889, 29889]
debug val_gt 128 [16387, 10943, 10548, 16387, 443, 10943, 14831, 16387, 10943, 10548]
Traceback (most recent call last):                                                                                                                                                | 0/7 [00:00<?, ?it/s]
  File "./open_instruct/open_instruct/finetune_val.py", line 923, in <module>
    main()
  File "./open_instruct/open_instruct/finetune_val.py", line 844, in main
    scores_metric.add_batch(predictions=batch_predict, references=val_gt)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/datasets/metric.py", line 494, in add_batch
    batch = self.info.features.encode_batch(batch)
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/datasets/features/features.py", line 1888, in encode_batch
    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/datasets/features/features.py", line 1888, in <listcomp>
    encoded_batch[key] = [encode_nested_example(self[key], obj) for obj in column]
  File "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/datasets/features/features.py", line 1273, in encode_nested_example
    if len(obj) > 0:
TypeError: object of type 'int' has no len()