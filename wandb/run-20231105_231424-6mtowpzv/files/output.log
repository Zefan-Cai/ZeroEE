11/05/2023 23:14:27 - INFO - __main__ - ***** Running training *****
11/05/2023 23:14:27 - INFO - __main__ -   Num examples = 341968
11/05/2023 23:14:27 - INFO - __main__ -   Num Epochs = 2
11/05/2023 23:14:27 - INFO - __main__ -   Instantaneous batch size per device = 1
11/05/2023 23:14:27 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4
11/05/2023 23:14:27 - INFO - __main__ -   Gradient Accumulation steps = 1
11/05/2023 23:14:27 - INFO - __main__ -   Total optimization steps = 170984
  0%|                                                                                                                                                                        | 0/170984 [00:00<?, ?it/s]

  0%|                                                                                                                                                            | 1/170984 [00:04<209:41:00,  4.41s/it]11/05/2023 23:14:32 - INFO - __main__ -   Step: 1, LR: 3.899015498586607e-09, Loss: 5.033697128295898
[2023-11-05 23:14:35,518] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 2/170984 [00:07<177:00:37,  3.73s/it]11/05/2023 23:14:35 - INFO - __main__ -   Step: 2, LR: 7.798030997173214e-09, Loss: 5.581179618835449
[2023-11-05 23:14:38,751] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 3/170984 [00:10<166:18:12,  3.50s/it]11/05/2023 23:14:38 - INFO - __main__ -   Step: 3, LR: 1.1697046495759823e-08, Loss: 5.156863689422607
  0%|                                                                                                                                                            | 4/170984 [00:14<162:39:26,  3.42s/it]11/05/2023 23:14:42 - INFO - __main__ -   Step: 4, LR: 1.559606199434643e-08, Loss: 5.421165943145752
[2023-11-05 23:14:42,058] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:14:45,309] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 5/170984 [00:17<159:40:34,  3.36s/it]11/05/2023 23:14:45 - INFO - __main__ -   Step: 5, LR: 1.9495077492933035e-08, Loss: 5.389344215393066
[2023-11-05 23:14:48,591] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 6/170984 [00:20<158:23:22,  3.33s/it]11/05/2023 23:14:48 - INFO - __main__ -   Step: 6, LR: 2.3394092991519646e-08, Loss: 4.919587135314941
[2023-11-05 23:14:51,734] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 7/170984 [00:23<155:24:25,  3.27s/it]11/05/2023 23:14:51 - INFO - __main__ -   Step: 7, LR: 2.729310849010625e-08, Loss: 5.1064863204956055
  0%|                                                                                                                                                            | 8/170984 [00:27<154:37:25,  3.26s/it]11/05/2023 23:14:54 - INFO - __main__ -   Step: 8, LR: 3.119212398869286e-08, Loss: 5.344651222229004
[2023-11-05 23:14:54,955] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                            | 9/170984 [00:30<155:09:45,  3.27s/it]11/05/2023 23:14:58 - INFO - __main__ -   Step: 9, LR: 3.509113948727947e-08, Loss: 4.988048076629639
[2023-11-05 23:14:58,246] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 10/170984 [00:33<155:07:36,  3.27s/it]11/05/2023 23:15:01 - INFO - __main__ -   Step: 10, LR: 3.899015498586607e-08, Loss: 5.097675800323486
[2023-11-05 23:15:01,512] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 11/170984 [00:37<156:28:39,  3.29s/it]11/05/2023 23:15:04 - INFO - __main__ -   Step: 11, LR: 4.288917048445268e-08, Loss: 5.239658355712891
[2023-11-05 23:15:04,871] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 12/170984 [00:40<157:20:53,  3.31s/it]11/05/2023 23:15:08 - INFO - __main__ -   Step: 12, LR: 4.678818598303929e-08, Loss: 5.920657157897949
[2023-11-05 23:15:08,226] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 13/170984 [00:43<157:01:20,  3.31s/it]11/05/2023 23:15:11 - INFO - __main__ -   Step: 13, LR: 5.068720148162589e-08, Loss: 5.133365154266357
[2023-11-05 23:15:11,517] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 14/170984 [00:46<156:11:57,  3.29s/it]11/05/2023 23:15:14 - INFO - __main__ -   Step: 14, LR: 5.45862169802125e-08, Loss: 4.998332500457764
[2023-11-05 23:15:14,765] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 15/170984 [00:50<158:59:39,  3.35s/it]11/05/2023 23:15:18 - INFO - __main__ -   Step: 15, LR: 5.848523247879911e-08, Loss: 5.7988433837890625
[2023-11-05 23:15:18,250] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 16/170984 [00:53<157:54:37,  3.33s/it]11/05/2023 23:15:21 - INFO - __main__ -   Step: 16, LR: 6.238424797738571e-08, Loss: 6.429396629333496
[2023-11-05 23:15:21,522] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 17/170984 [00:56<157:48:37,  3.32s/it]11/05/2023 23:15:24 - INFO - __main__ -   Step: 17, LR: 6.628326347597232e-08, Loss: 5.139513969421387
[2023-11-05 23:15:24,840] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 18/170984 [01:00<158:31:19,  3.34s/it]11/05/2023 23:15:28 - INFO - __main__ -   Step: 18, LR: 7.018227897455893e-08, Loss: 5.506174087524414
[2023-11-05 23:15:28,213] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:15:31,501] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 19/170984 [01:03<157:49:01,  3.32s/it]11/05/2023 23:15:31 - INFO - __main__ -   Step: 19, LR: 7.408129447314553e-08, Loss: 5.097649097442627
[2023-11-05 23:15:34,814] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 20/170984 [01:06<157:39:52,  3.32s/it]11/05/2023 23:15:34 - INFO - __main__ -   Step: 20, LR: 7.798030997173214e-08, Loss: 4.934576988220215
[2023-11-05 23:15:38,156] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 21/170984 [01:10<157:58:26,  3.33s/it]11/05/2023 23:15:38 - INFO - __main__ -   Step: 21, LR: 8.187932547031875e-08, Loss: 3.8108439445495605
[2023-11-05 23:15:41,466] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 22/170984 [01:13<157:44:16,  3.32s/it]11/05/2023 23:15:41 - INFO - __main__ -   Step: 22, LR: 8.577834096890536e-08, Loss: 5.059563636779785
[2023-11-05 23:15:44,827] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 23/170984 [01:16<158:18:08,  3.33s/it]11/05/2023 23:15:44 - INFO - __main__ -   Step: 23, LR: 8.967735646749196e-08, Loss: 5.425708770751953
[2023-11-05 23:15:48,151] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 24/170984 [01:20<158:09:43,  3.33s/it]11/05/2023 23:15:48 - INFO - __main__ -   Step: 24, LR: 9.357637196607858e-08, Loss: 5.460511207580566
[2023-11-05 23:15:51,498] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 25/170984 [01:23<158:24:24,  3.34s/it]11/05/2023 23:15:51 - INFO - __main__ -   Step: 25, LR: 9.747538746466518e-08, Loss: 5.076367378234863
[2023-11-05 23:15:54,770] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 26/170984 [01:26<157:29:33,  3.32s/it]11/05/2023 23:15:54 - INFO - __main__ -   Step: 26, LR: 1.0137440296325178e-07, Loss: 4.361335277557373
[2023-11-05 23:15:58,198] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 27/170984 [01:30<159:05:03,  3.35s/it]11/05/2023 23:15:58 - INFO - __main__ -   Step: 27, LR: 1.052734184618384e-07, Loss: 5.084969520568848
[2023-11-05 23:16:01,546] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 28/170984 [01:33<159:03:09,  3.35s/it]11/05/2023 23:16:01 - INFO - __main__ -   Step: 28, LR: 1.09172433960425e-07, Loss: 5.170012474060059
[2023-11-05 23:16:04,771] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 29/170984 [01:36<157:16:32,  3.31s/it]11/05/2023 23:16:04 - INFO - __main__ -   Step: 29, LR: 1.1307144945901161e-07, Loss: 4.756167411804199
[2023-11-05 23:16:08,049] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 30/170984 [01:40<156:47:24,  3.30s/it]11/05/2023 23:16:08 - INFO - __main__ -   Step: 30, LR: 1.1697046495759822e-07, Loss: 3.8948006629943848
[2023-11-05 23:16:11,423] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 31/170984 [01:43<157:49:13,  3.32s/it]11/05/2023 23:16:11 - INFO - __main__ -   Step: 31, LR: 1.2086948045618483e-07, Loss: 5.1379594802856445
[2023-11-05 23:16:14,876] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 32/170984 [01:47<159:40:16,  3.36s/it]11/05/2023 23:16:14 - INFO - __main__ -   Step: 32, LR: 1.2476849595477143e-07, Loss: 4.795962333679199
[2023-11-05 23:16:18,197] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 33/170984 [01:50<159:04:24,  3.35s/it]11/05/2023 23:16:18 - INFO - __main__ -   Step: 33, LR: 1.2866751145335805e-07, Loss: 5.279245376586914
[2023-11-05 23:16:21,512] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 34/170984 [01:53<158:34:36,  3.34s/it]11/05/2023 23:16:21 - INFO - __main__ -   Step: 34, LR: 1.3256652695194465e-07, Loss: 4.440101623535156
[2023-11-05 23:16:24,737] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 35/170984 [01:56<156:57:21,  3.31s/it]11/05/2023 23:16:24 - INFO - __main__ -   Step: 35, LR: 1.3646554245053124e-07, Loss: 4.855957984924316
[2023-11-05 23:16:27,943] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 36/170984 [02:00<155:32:08,  3.28s/it]11/05/2023 23:16:27 - INFO - __main__ -   Step: 36, LR: 1.4036455794911787e-07, Loss: 4.684963226318359
[2023-11-05 23:16:31,189] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 37/170984 [02:03<155:06:43,  3.27s/it]11/05/2023 23:16:31 - INFO - __main__ -   Step: 37, LR: 1.4426357344770447e-07, Loss: 4.419365882873535
[2023-11-05 23:16:34,408] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 38/170984 [02:06<154:25:52,  3.25s/it]11/05/2023 23:16:34 - INFO - __main__ -   Step: 38, LR: 1.4816258894629106e-07, Loss: 4.693793296813965
[2023-11-05 23:16:37,770] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 39/170984 [02:09<155:59:21,  3.29s/it]11/05/2023 23:16:37 - INFO - __main__ -   Step: 39, LR: 1.5206160444487769e-07, Loss: 4.107207298278809
  0%|                                                                                                                                                           | 40/170984 [02:13<157:05:29,  3.31s/it]11/05/2023 23:16:41 - INFO - __main__ -   Step: 40, LR: 1.5596061994346428e-07, Loss: 4.399111270904541
[2023-11-05 23:16:41,132] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 41/170984 [02:16<157:45:07,  3.32s/it]11/05/2023 23:16:44 - INFO - __main__ -   Step: 41, LR: 1.598596354420509e-07, Loss: 4.767019271850586
[2023-11-05 23:16:44,487] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 42/170984 [02:19<156:32:58,  3.30s/it]11/05/2023 23:16:47 - INFO - __main__ -   Step: 42, LR: 1.637586509406375e-07, Loss: 4.868919372558594
[2023-11-05 23:16:47,725] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 43/170984 [02:23<156:35:10,  3.30s/it]11/05/2023 23:16:51 - INFO - __main__ -   Step: 43, LR: 1.676576664392241e-07, Loss: 4.406726837158203
[2023-11-05 23:16:51,024] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 44/170984 [02:26<156:35:38,  3.30s/it]11/05/2023 23:16:54 - INFO - __main__ -   Step: 44, LR: 1.7155668193781073e-07, Loss: 4.541374206542969
[2023-11-05 23:16:54,322] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 45/170984 [02:29<156:16:10,  3.29s/it]11/05/2023 23:16:57 - INFO - __main__ -   Step: 45, LR: 1.7545569743639732e-07, Loss: 4.128124237060547
[2023-11-05 23:16:57,598] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 46/170984 [02:33<155:57:34,  3.28s/it]11/05/2023 23:17:00 - INFO - __main__ -   Step: 46, LR: 1.7935471293498392e-07, Loss: 4.272219657897949
[2023-11-05 23:17:00,867] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 47/170984 [02:36<155:18:56,  3.27s/it]11/05/2023 23:17:04 - INFO - __main__ -   Step: 47, LR: 1.8325372843357054e-07, Loss: 4.696451187133789
[2023-11-05 23:17:04,106] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 48/170984 [02:39<155:31:53,  3.28s/it]11/05/2023 23:17:07 - INFO - __main__ -   Step: 48, LR: 1.8715274393215717e-07, Loss: 4.2285237312316895
[2023-11-05 23:17:07,393] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:17:10,914] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 49/170984 [02:43<159:01:57,  3.35s/it]11/05/2023 23:17:10 - INFO - __main__ -   Step: 49, LR: 1.9105175943074376e-07, Loss: 3.8893613815307617
[2023-11-05 23:17:14,191] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 50/170984 [02:46<158:00:27,  3.33s/it]11/05/2023 23:17:14 - INFO - __main__ -   Step: 50, LR: 1.9495077492933036e-07, Loss: 4.399304389953613
[2023-11-05 23:17:17,475] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 51/170984 [02:49<157:22:52,  3.31s/it]11/05/2023 23:17:17 - INFO - __main__ -   Step: 51, LR: 1.9884979042791696e-07, Loss: 4.069501876831055
[2023-11-05 23:17:20,732] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 52/170984 [02:52<156:33:40,  3.30s/it]11/05/2023 23:17:20 - INFO - __main__ -   Step: 52, LR: 2.0274880592650356e-07, Loss: 3.829094886779785
[2023-11-05 23:17:24,046] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 53/170984 [02:56<156:47:11,  3.30s/it]11/05/2023 23:17:24 - INFO - __main__ -   Step: 53, LR: 2.066478214250902e-07, Loss: 4.015111923217773
[2023-11-05 23:17:27,407] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 54/170984 [02:59<157:38:01,  3.32s/it]11/05/2023 23:17:27 - INFO - __main__ -   Step: 54, LR: 2.105468369236768e-07, Loss: 4.046903133392334
[2023-11-05 23:17:30,668] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 55/170984 [03:02<156:47:21,  3.30s/it]11/05/2023 23:17:30 - INFO - __main__ -   Step: 55, LR: 2.144458524222634e-07, Loss: 3.6456546783447266
[2023-11-05 23:17:33,950] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 56/170984 [03:06<156:30:16,  3.30s/it]11/05/2023 23:17:33 - INFO - __main__ -   Step: 56, LR: 2.1834486792085e-07, Loss: 3.9880919456481934
[2023-11-05 23:17:37,163] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 57/170984 [03:09<155:19:12,  3.27s/it]11/05/2023 23:17:37 - INFO - __main__ -   Step: 57, LR: 2.222438834194366e-07, Loss: 3.7157936096191406
[2023-11-05 23:17:40,432] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 58/170984 [03:12<155:17:13,  3.27s/it]11/05/2023 23:17:40 - INFO - __main__ -   Step: 58, LR: 2.2614289891802322e-07, Loss: 3.7903530597686768
[2023-11-05 23:17:43,739] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 59/170984 [03:15<155:48:05,  3.28s/it]11/05/2023 23:17:43 - INFO - __main__ -   Step: 59, LR: 2.3004191441660984e-07, Loss: 4.0037689208984375
[2023-11-05 23:17:47,022] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 60/170984 [03:19<155:49:24,  3.28s/it]11/05/2023 23:17:47 - INFO - __main__ -   Step: 60, LR: 2.3394092991519644e-07, Loss: 4.24681282043457
[2023-11-05 23:17:50,337] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 61/170984 [03:22<156:17:07,  3.29s/it]11/05/2023 23:17:50 - INFO - __main__ -   Step: 61, LR: 2.3783994541378304e-07, Loss: 3.507457733154297
[2023-11-05 23:17:53,649] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 62/170984 [03:25<156:34:14,  3.30s/it]11/05/2023 23:17:53 - INFO - __main__ -   Step: 62, LR: 2.4173896091236966e-07, Loss: 3.695502996444702
  0%|                                                                                                                                                           | 63/170984 [03:29<156:39:03,  3.30s/it]11/05/2023 23:17:56 - INFO - __main__ -   Step: 63, LR: 2.4563797641095623e-07, Loss: 3.3405396938323975
[2023-11-05 23:17:56,952] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 64/170984 [03:32<156:30:45,  3.30s/it]11/05/2023 23:18:00 - INFO - __main__ -   Step: 64, LR: 2.4953699190954285e-07, Loss: 3.2451329231262207
[2023-11-05 23:18:00,242] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 65/170984 [03:35<157:10:17,  3.31s/it]11/05/2023 23:18:03 - INFO - __main__ -   Step: 65, LR: 2.534360074081295e-07, Loss: 3.4773688316345215
[2023-11-05 23:18:03,585] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 66/170984 [03:39<157:37:37,  3.32s/it]11/05/2023 23:18:06 - INFO - __main__ -   Step: 66, LR: 2.573350229067161e-07, Loss: 2.9825382232666016
[2023-11-05 23:18:06,927] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 67/170984 [03:42<158:37:49,  3.34s/it]11/05/2023 23:18:10 - INFO - __main__ -   Step: 67, LR: 2.6123403840530267e-07, Loss: 2.7842445373535156
[2023-11-05 23:18:10,318] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 68/170984 [03:45<157:43:52,  3.32s/it]11/05/2023 23:18:13 - INFO - __main__ -   Step: 68, LR: 2.651330539038893e-07, Loss: 2.711982488632202
[2023-11-05 23:18:13,596] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 69/170984 [03:49<157:39:33,  3.32s/it]11/05/2023 23:18:16 - INFO - __main__ -   Step: 69, LR: 2.690320694024759e-07, Loss: 2.704106330871582
[2023-11-05 23:18:16,913] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 70/170984 [03:52<158:39:06,  3.34s/it]11/05/2023 23:18:20 - INFO - __main__ -   Step: 70, LR: 2.729310849010625e-07, Loss: 2.6365342140197754
[2023-11-05 23:18:20,303] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 71/170984 [03:55<158:02:56,  3.33s/it]11/05/2023 23:18:23 - INFO - __main__ -   Step: 71, LR: 2.768301003996491e-07, Loss: 2.572814702987671
[2023-11-05 23:18:23,603] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 72/170984 [03:59<157:47:52,  3.32s/it]11/05/2023 23:18:26 - INFO - __main__ -   Step: 72, LR: 2.8072911589823574e-07, Loss: 2.364339828491211
[2023-11-05 23:18:26,915] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 73/170984 [04:02<156:55:03,  3.31s/it]11/05/2023 23:18:30 - INFO - __main__ -   Step: 73, LR: 2.846281313968223e-07, Loss: 2.4532275199890137
[2023-11-05 23:18:30,177] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 74/170984 [04:05<157:16:45,  3.31s/it]11/05/2023 23:18:33 - INFO - __main__ -   Step: 74, LR: 2.8852714689540893e-07, Loss: 2.220689535140991
[2023-11-05 23:18:33,507] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 75/170984 [04:08<155:42:17,  3.28s/it]11/05/2023 23:18:36 - INFO - __main__ -   Step: 75, LR: 2.9242616239399556e-07, Loss: 2.0115246772766113
[2023-11-05 23:18:36,710] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 76/170984 [04:12<155:20:18,  3.27s/it]11/05/2023 23:18:39 - INFO - __main__ -   Step: 76, LR: 2.963251778925821e-07, Loss: 1.837895393371582
[2023-11-05 23:18:39,964] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 77/170984 [04:15<156:48:09,  3.30s/it]11/05/2023 23:18:43 - INFO - __main__ -   Step: 77, LR: 3.0022419339116875e-07, Loss: 2.0042150020599365
[2023-11-05 23:18:43,339] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 78/170984 [04:18<157:14:27,  3.31s/it]11/05/2023 23:18:46 - INFO - __main__ -   Step: 78, LR: 3.0412320888975537e-07, Loss: 1.7988297939300537
[2023-11-05 23:18:46,673] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 79/170984 [04:22<157:17:03,  3.31s/it]11/05/2023 23:18:49 - INFO - __main__ -   Step: 79, LR: 3.08022224388342e-07, Loss: 1.2199127674102783
[2023-11-05 23:18:49,988] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 80/170984 [04:25<157:23:49,  3.32s/it]11/05/2023 23:18:53 - INFO - __main__ -   Step: 80, LR: 3.1192123988692857e-07, Loss: 1.2896292209625244
[2023-11-05 23:18:53,309] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 81/170984 [04:28<157:28:34,  3.32s/it]11/05/2023 23:18:56 - INFO - __main__ -   Step: 81, LR: 3.158202553855152e-07, Loss: 0.9715279340744019
[2023-11-05 23:18:56,630] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 82/170984 [04:32<156:42:57,  3.30s/it]11/05/2023 23:18:59 - INFO - __main__ -   Step: 82, LR: 3.197192708841018e-07, Loss: 0.8822070360183716
[2023-11-05 23:18:59,894] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 83/170984 [04:35<157:50:49,  3.33s/it]11/05/2023 23:19:03 - INFO - __main__ -   Step: 83, LR: 3.236182863826884e-07, Loss: 0.6021388173103333
[2023-11-05 23:19:03,275] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 84/170984 [04:38<157:07:45,  3.31s/it]11/05/2023 23:19:06 - INFO - __main__ -   Step: 84, LR: 3.27517301881275e-07, Loss: 0.8238840699195862
[2023-11-05 23:19:06,549] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 85/170984 [04:41<156:22:12,  3.29s/it]11/05/2023 23:19:09 - INFO - __main__ -   Step: 85, LR: 3.314163173798616e-07, Loss: 1.261202335357666
[2023-11-05 23:19:09,806] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 86/170984 [04:45<156:43:36,  3.30s/it]11/05/2023 23:19:13 - INFO - __main__ -   Step: 86, LR: 3.353153328784482e-07, Loss: 0.33282196521759033
[2023-11-05 23:19:13,125] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 87/170984 [04:48<157:11:37,  3.31s/it]11/05/2023 23:19:16 - INFO - __main__ -   Step: 87, LR: 3.392143483770349e-07, Loss: 0.33610033988952637
[2023-11-05 23:19:16,459] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 88/170984 [04:51<157:06:47,  3.31s/it]11/05/2023 23:19:19 - INFO - __main__ -   Step: 88, LR: 3.4311336387562145e-07, Loss: 0.3451862335205078
[2023-11-05 23:19:19,765] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 89/170984 [04:55<156:50:07,  3.30s/it]11/05/2023 23:19:23 - INFO - __main__ -   Step: 89, LR: 3.470123793742081e-07, Loss: 0.1684451550245285
[2023-11-05 23:19:23,055] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 90/170984 [04:58<157:30:44,  3.32s/it]11/05/2023 23:19:26 - INFO - __main__ -   Step: 90, LR: 3.5091139487279464e-07, Loss: 0.253866046667099
[2023-11-05 23:19:26,407] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 91/170984 [05:01<157:32:13,  3.32s/it]11/05/2023 23:19:29 - INFO - __main__ -   Step: 91, LR: 3.5481041037138127e-07, Loss: 0.26484614610671997
[2023-11-05 23:19:29,727] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 92/170984 [05:05<157:57:24,  3.33s/it]11/05/2023 23:19:33 - INFO - __main__ -   Step: 92, LR: 3.5870942586996784e-07, Loss: 0.15056772530078888
[2023-11-05 23:19:33,075] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 93/170984 [05:08<157:15:16,  3.31s/it]11/05/2023 23:19:36 - INFO - __main__ -   Step: 93, LR: 3.6260844136855446e-07, Loss: 0.06864014267921448
[2023-11-05 23:19:36,353] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 94/170984 [05:11<159:04:09,  3.35s/it]11/05/2023 23:19:39 - INFO - __main__ -   Step: 94, LR: 3.665074568671411e-07, Loss: 0.03596308082342148
[2023-11-05 23:19:39,793] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 95/170984 [05:15<157:41:43,  3.32s/it]11/05/2023 23:19:43 - INFO - __main__ -   Step: 95, LR: 3.7040647236572766e-07, Loss: 0.5381619334220886
[2023-11-05 23:19:43,048] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 96/170984 [05:18<157:16:27,  3.31s/it]11/05/2023 23:19:46 - INFO - __main__ -   Step: 96, LR: 3.7430548786431433e-07, Loss: 0.02208741009235382
[2023-11-05 23:19:46,340] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 97/170984 [05:21<156:17:41,  3.29s/it]11/05/2023 23:19:49 - INFO - __main__ -   Step: 97, LR: 3.7820450336290085e-07, Loss: 0.5264633893966675
[2023-11-05 23:19:49,585] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 98/170984 [05:25<156:47:37,  3.30s/it]11/05/2023 23:19:52 - INFO - __main__ -   Step: 98, LR: 3.8210351886148753e-07, Loss: 0.010169602930545807
[2023-11-05 23:19:52,913] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                           | 99/170984 [05:28<158:07:27,  3.33s/it]11/05/2023 23:19:56 - INFO - __main__ -   Step: 99, LR: 3.860025343600741e-07, Loss: 1.0449763536453247
[2023-11-05 23:19:56,309] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 100/170984 [05:31<159:49:47,  3.37s/it]11/05/2023 23:19:59 - INFO - __main__ -   Step: 100, LR: 3.899015498586607e-07, Loss: 0.008767439983785152
[2023-11-05 23:19:59,760] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 101/170984 [05:35<159:48:30,  3.37s/it]11/05/2023 23:20:03 - INFO - __main__ -   Step: 101, LR: 3.9380056535724735e-07, Loss: 0.4168277978897095
[2023-11-05 23:20:03,126] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 102/170984 [05:38<159:44:29,  3.37s/it]11/05/2023 23:20:06 - INFO - __main__ -   Step: 102, LR: 3.976995808558339e-07, Loss: 0.4561440050601959
[2023-11-05 23:20:06,488] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 103/170984 [05:41<158:44:06,  3.34s/it]11/05/2023 23:20:09 - INFO - __main__ -   Step: 103, LR: 4.0159859635442054e-07, Loss: 0.005675734952092171
[2023-11-05 23:20:09,783] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 104/170984 [05:45<158:17:36,  3.33s/it]11/05/2023 23:20:13 - INFO - __main__ -   Step: 104, LR: 4.054976118530071e-07, Loss: 0.0046572210267186165
[2023-11-05 23:20:13,096] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 105/170984 [05:48<158:52:35,  3.35s/it]11/05/2023 23:20:16 - INFO - __main__ -   Step: 105, LR: 4.0939662735159373e-07, Loss: 0.6102954745292664
[2023-11-05 23:20:16,472] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 106/170984 [05:51<157:22:34,  3.32s/it]11/05/2023 23:20:19 - INFO - __main__ -   Step: 106, LR: 4.132956428501804e-07, Loss: 0.0038060909137129784
[2023-11-05 23:20:19,713] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 107/170984 [05:55<156:41:08,  3.30s/it]11/05/2023 23:20:22 - INFO - __main__ -   Step: 107, LR: 4.17194658348767e-07, Loss: 0.0019273238722234964
[2023-11-05 23:20:22,981] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 108/170984 [05:58<156:44:29,  3.30s/it]11/05/2023 23:20:26 - INFO - __main__ -   Step: 108, LR: 4.210936738473536e-07, Loss: 0.004696570802479982
[2023-11-05 23:20:26,286] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 109/170984 [06:01<158:21:44,  3.34s/it]11/05/2023 23:20:29 - INFO - __main__ -   Step: 109, LR: 4.249926893459402e-07, Loss: 0.0022272851783782244
[2023-11-05 23:20:29,702] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 110/170984 [06:05<156:04:26,  3.29s/it]11/05/2023 23:20:32 - INFO - __main__ -   Step: 110, LR: 4.288917048445268e-07, Loss: 0.001338917762041092
[2023-11-05 23:20:32,877] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 111/170984 [06:08<156:32:23,  3.30s/it]11/05/2023 23:20:36 - INFO - __main__ -   Step: 111, LR: 4.3279072034311337e-07, Loss: 0.0022056412417441607
[2023-11-05 23:20:36,198] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:20:39,601] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 112/170984 [06:11<158:01:36,  3.33s/it]11/05/2023 23:20:39 - INFO - __main__ -   Step: 112, LR: 4.366897358417e-07, Loss: 0.4761475920677185
  0%|                                                                                                                                                          | 113/170984 [06:15<161:02:00,  3.39s/it]11/05/2023 23:20:43 - INFO - __main__ -   Step: 113, LR: 4.405887513402866e-07, Loss: 0.36930951476097107
[2023-11-05 23:20:43,142] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 114/170984 [06:18<160:08:30,  3.37s/it]11/05/2023 23:20:46 - INFO - __main__ -   Step: 114, LR: 4.444877668388732e-07, Loss: 0.5800454616546631
[2023-11-05 23:20:46,472] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 115/170984 [06:21<159:40:25,  3.36s/it]11/05/2023 23:20:49 - INFO - __main__ -   Step: 115, LR: 4.4838678233745987e-07, Loss: 0.001189729431644082
[2023-11-05 23:20:49,813] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 116/170984 [06:25<159:46:39,  3.37s/it]11/05/2023 23:20:53 - INFO - __main__ -   Step: 116, LR: 4.5228579783604644e-07, Loss: 0.0006483363104052842
[2023-11-05 23:20:53,184] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 117/170984 [06:28<161:16:19,  3.40s/it]11/05/2023 23:20:56 - INFO - __main__ -   Step: 117, LR: 4.5618481333463306e-07, Loss: 0.30624279379844666
[2023-11-05 23:20:56,656] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 118/170984 [06:32<159:34:54,  3.36s/it]11/05/2023 23:20:59 - INFO - __main__ -   Step: 118, LR: 4.600838288332197e-07, Loss: 0.3059220314025879
[2023-11-05 23:20:59,935] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 119/170984 [06:35<158:39:17,  3.34s/it]11/05/2023 23:21:03 - INFO - __main__ -   Step: 119, LR: 4.6398284433180625e-07, Loss: 0.2897004187107086
[2023-11-05 23:21:03,232] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 120/170984 [06:38<158:36:00,  3.34s/it]11/05/2023 23:21:06 - INFO - __main__ -   Step: 120, LR: 4.678818598303929e-07, Loss: 0.9173391461372375
[2023-11-05 23:21:06,571] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 121/170984 [06:42<159:44:06,  3.37s/it]11/05/2023 23:21:09 - INFO - __main__ -   Step: 121, LR: 4.7178087532897945e-07, Loss: 0.014437069185078144
[2023-11-05 23:21:09,993] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 122/170984 [06:45<159:20:45,  3.36s/it]11/05/2023 23:21:13 - INFO - __main__ -   Step: 122, LR: 4.7567989082756607e-07, Loss: 0.1381407231092453
[2023-11-05 23:21:13,331] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 123/170984 [06:48<158:15:10,  3.33s/it]11/05/2023 23:21:16 - INFO - __main__ -   Step: 123, LR: 4.795789063261526e-07, Loss: 0.11756136268377304
[2023-11-05 23:21:16,611] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 124/170984 [06:52<159:31:56,  3.36s/it]11/05/2023 23:21:20 - INFO - __main__ -   Step: 124, LR: 4.834779218247393e-07, Loss: 0.2581769824028015
[2023-11-05 23:21:20,036] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 125/170984 [06:55<159:12:38,  3.35s/it]11/05/2023 23:21:23 - INFO - __main__ -   Step: 125, LR: 4.873769373233259e-07, Loss: 0.3704734444618225
[2023-11-05 23:21:23,374] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 126/170984 [06:58<158:12:39,  3.33s/it]11/05/2023 23:21:26 - INFO - __main__ -   Step: 126, LR: 4.912759528219125e-07, Loss: 0.24950769543647766
[2023-11-05 23:21:26,658] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 127/170984 [07:02<158:21:59,  3.34s/it]11/05/2023 23:21:30 - INFO - __main__ -   Step: 127, LR: 4.951749683204991e-07, Loss: 0.013901911675930023
[2023-11-05 23:21:30,003] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 128/170984 [07:05<158:21:53,  3.34s/it]11/05/2023 23:21:33 - INFO - __main__ -   Step: 128, LR: 4.990739838190857e-07, Loss: 0.04700302705168724
[2023-11-05 23:21:33,340] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 129/170984 [07:08<158:14:21,  3.33s/it]11/05/2023 23:21:36 - INFO - __main__ -   Step: 129, LR: 5.029729993176724e-07, Loss: 0.08127212524414062
[2023-11-05 23:21:36,668] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 130/170984 [07:12<156:36:15,  3.30s/it]11/05/2023 23:21:39 - INFO - __main__ -   Step: 130, LR: 5.06872014816259e-07, Loss: 0.0962289422750473
[2023-11-05 23:21:39,887] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 131/170984 [07:15<156:56:37,  3.31s/it]11/05/2023 23:21:43 - INFO - __main__ -   Step: 131, LR: 5.107710303148455e-07, Loss: 0.024464888498187065
[2023-11-05 23:21:43,212] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 132/170984 [07:18<159:01:09,  3.35s/it]11/05/2023 23:21:46 - INFO - __main__ -   Step: 132, LR: 5.146700458134322e-07, Loss: 0.22361434996128082
[2023-11-05 23:21:46,664] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 133/170984 [07:22<159:11:06,  3.35s/it]11/05/2023 23:21:50 - INFO - __main__ -   Step: 133, LR: 5.185690613120188e-07, Loss: 0.0038320128805935383
[2023-11-05 23:21:50,026] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 134/170984 [07:25<158:31:52,  3.34s/it]11/05/2023 23:21:53 - INFO - __main__ -   Step: 134, LR: 5.224680768106053e-07, Loss: 0.5142015218734741
[2023-11-05 23:21:53,335] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 135/170984 [07:28<158:39:47,  3.34s/it]11/05/2023 23:21:56 - INFO - __main__ -   Step: 135, LR: 5.263670923091919e-07, Loss: 0.0013497673207893968
[2023-11-05 23:21:56,685] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 136/170984 [07:32<158:27:01,  3.34s/it]11/05/2023 23:22:00 - INFO - __main__ -   Step: 136, LR: 5.302661078077786e-07, Loss: 0.0518503412604332
[2023-11-05 23:22:00,013] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 137/170984 [07:35<158:26:46,  3.34s/it]11/05/2023 23:22:03 - INFO - __main__ -   Step: 137, LR: 5.341651233063653e-07, Loss: 0.6591722965240479
[2023-11-05 23:22:03,352] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                          | 138/170984 [07:38<159:38:21,  3.36s/it]11/05/2023 23:22:06 - INFO - __main__ -   Step: 138, LR: 5.380641388049518e-07, Loss: 0.003481869585812092
[2023-11-05 23:22:06,774] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 139/170984 [07:42<159:57:22,  3.37s/it]11/05/2023 23:22:10 - INFO - __main__ -   Step: 139, LR: 5.419631543035384e-07, Loss: 0.0021308143623173237
[2023-11-05 23:22:10,160] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:22:13,506] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 140/170984 [07:45<159:35:38,  3.36s/it]11/05/2023 23:22:13 - INFO - __main__ -   Step: 140, LR: 5.45862169802125e-07, Loss: 0.6951427459716797
  0%|                                                                                                                                                         | 141/170984 [07:49<159:30:19,  3.36s/it]11/05/2023 23:22:16 - INFO - __main__ -   Step: 141, LR: 5.497611853007117e-07, Loss: 0.0007623739656992257
[2023-11-05 23:22:16,862] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 142/170984 [07:52<159:21:39,  3.36s/it]11/05/2023 23:22:20 - INFO - __main__ -   Step: 142, LR: 5.536602007992982e-07, Loss: 0.49023357033729553
[2023-11-05 23:22:20,213] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 143/170984 [07:55<159:11:04,  3.35s/it]11/05/2023 23:22:23 - INFO - __main__ -   Step: 143, LR: 5.575592162978848e-07, Loss: 0.0005779967177659273
[2023-11-05 23:22:23,559] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 144/170984 [07:59<158:33:22,  3.34s/it]11/05/2023 23:22:26 - INFO - __main__ -   Step: 144, LR: 5.614582317964715e-07, Loss: 0.0002609738730825484
[2023-11-05 23:22:26,869] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 145/170984 [08:02<158:07:28,  3.33s/it]11/05/2023 23:22:30 - INFO - __main__ -   Step: 145, LR: 5.65357247295058e-07, Loss: 0.0007093403255566955
[2023-11-05 23:22:30,180] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 146/170984 [08:05<158:41:15,  3.34s/it]11/05/2023 23:22:33 - INFO - __main__ -   Step: 146, LR: 5.692562627936446e-07, Loss: 0.0002721488708630204
[2023-11-05 23:22:33,552] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 147/170984 [08:09<158:30:37,  3.34s/it]11/05/2023 23:22:36 - INFO - __main__ -   Step: 147, LR: 5.731552782922312e-07, Loss: 0.0007202452979981899
[2023-11-05 23:22:36,884] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 148/170984 [08:12<157:46:03,  3.32s/it]11/05/2023 23:22:40 - INFO - __main__ -   Step: 148, LR: 5.770542937908179e-07, Loss: 0.31007879972457886
[2023-11-05 23:22:40,172] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 149/170984 [08:15<157:48:38,  3.33s/it]11/05/2023 23:22:43 - INFO - __main__ -   Step: 149, LR: 5.809533092894045e-07, Loss: 0.0004478204937186092
[2023-11-05 23:22:43,499] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 150/170984 [08:19<159:05:11,  3.35s/it]11/05/2023 23:22:46 - INFO - __main__ -   Step: 150, LR: 5.848523247879911e-07, Loss: 0.0007391598192043602
[2023-11-05 23:22:46,914] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 151/170984 [08:22<158:48:12,  3.35s/it]11/05/2023 23:22:50 - INFO - __main__ -   Step: 151, LR: 5.887513402865777e-07, Loss: 0.00014488495071418583
[2023-11-05 23:22:50,247] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 152/170984 [08:25<159:02:36,  3.35s/it]11/05/2023 23:22:53 - INFO - __main__ -   Step: 152, LR: 5.926503557851643e-07, Loss: 0.21767009794712067
[2023-11-05 23:22:53,611] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 153/170984 [08:29<159:08:54,  3.35s/it]11/05/2023 23:22:56 - INFO - __main__ -   Step: 153, LR: 5.965493712837509e-07, Loss: 0.48203784227371216
[2023-11-05 23:22:56,970] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 154/170984 [08:32<158:51:03,  3.35s/it]11/05/2023 23:23:00 - INFO - __main__ -   Step: 154, LR: 6.004483867823375e-07, Loss: 0.28140777349472046
[2023-11-05 23:23:00,303] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 155/170984 [08:35<157:58:14,  3.33s/it]11/05/2023 23:23:03 - INFO - __main__ -   Step: 155, LR: 6.043474022809241e-07, Loss: 0.6049352884292603
[2023-11-05 23:23:03,588] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 156/170984 [08:39<157:51:36,  3.33s/it]11/05/2023 23:23:06 - INFO - __main__ -   Step: 156, LR: 6.082464177795107e-07, Loss: 0.0018733199685811996
[2023-11-05 23:23:06,910] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 157/170984 [08:42<158:39:21,  3.34s/it]11/05/2023 23:23:10 - INFO - __main__ -   Step: 157, LR: 6.121454332780973e-07, Loss: 0.12716491520404816
[2023-11-05 23:23:10,292] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:23:13,672] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 158/170984 [08:45<159:11:59,  3.35s/it]11/05/2023 23:23:13 - INFO - __main__ -   Step: 158, LR: 6.16044448776684e-07, Loss: 0.0011713134590536356
  0%|                                                                                                                                                         | 159/170984 [08:49<160:49:20,  3.39s/it]11/05/2023 23:23:17 - INFO - __main__ -   Step: 159, LR: 6.199434642752706e-07, Loss: 0.3976067006587982
[2023-11-05 23:23:17,143] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 160/170984 [08:52<160:40:55,  3.39s/it]11/05/2023 23:23:20 - INFO - __main__ -   Step: 160, LR: 6.238424797738571e-07, Loss: 0.0029094640631228685
[2023-11-05 23:23:20,522] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 161/170984 [08:55<159:37:07,  3.36s/it]11/05/2023 23:23:23 - INFO - __main__ -   Step: 161, LR: 6.277414952724438e-07, Loss: 0.08862535655498505
[2023-11-05 23:23:23,834] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 162/170984 [08:59<158:04:39,  3.33s/it]11/05/2023 23:23:27 - INFO - __main__ -   Step: 162, LR: 6.316405107710304e-07, Loss: 0.04280346259474754
[2023-11-05 23:23:27,090] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 163/170984 [09:02<159:03:58,  3.35s/it]11/05/2023 23:23:30 - INFO - __main__ -   Step: 163, LR: 6.35539526269617e-07, Loss: 0.3501742482185364
[2023-11-05 23:23:30,490] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:23:33,896] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 164/170984 [09:06<159:49:24,  3.37s/it]11/05/2023 23:23:33 - INFO - __main__ -   Step: 164, LR: 6.394385417682036e-07, Loss: 0.4410953223705292
  0%|                                                                                                                                                         | 165/170984 [09:09<160:31:07,  3.38s/it]11/05/2023 23:23:37 - INFO - __main__ -   Step: 165, LR: 6.433375572667902e-07, Loss: 0.03373724967241287
[2023-11-05 23:23:37,314] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 166/170984 [09:12<159:03:32,  3.35s/it]11/05/2023 23:23:40 - INFO - __main__ -   Step: 166, LR: 6.472365727653768e-07, Loss: 0.21574944257736206
[2023-11-05 23:23:40,594] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:23:43,872] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 167/170984 [09:16<158:00:20,  3.33s/it]11/05/2023 23:23:43 - INFO - __main__ -   Step: 167, LR: 6.511355882639633e-07, Loss: 0.00037795602111145854
  0%|                                                                                                                                                         | 168/170984 [09:19<159:44:18,  3.37s/it]11/05/2023 23:23:47 - INFO - __main__ -   Step: 168, LR: 6.5503460376255e-07, Loss: 0.035163700580596924
[2023-11-05 23:23:47,324] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 169/170984 [09:22<159:32:49,  3.36s/it]11/05/2023 23:23:50 - INFO - __main__ -   Step: 169, LR: 6.589336192611366e-07, Loss: 0.03945901244878769
[2023-11-05 23:23:50,676] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:23:54,029] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 170/170984 [09:26<159:23:44,  3.36s/it]11/05/2023 23:23:54 - INFO - __main__ -   Step: 170, LR: 6.628326347597232e-07, Loss: 0.1287328451871872
  0%|                                                                                                                                                         | 171/170984 [09:29<159:26:36,  3.36s/it]11/05/2023 23:23:57 - INFO - __main__ -   Step: 171, LR: 6.667316502583098e-07, Loss: 0.017058562487363815
[2023-11-05 23:23:57,392] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 172/170984 [09:32<159:18:38,  3.36s/it]11/05/2023 23:24:00 - INFO - __main__ -   Step: 172, LR: 6.706306657568964e-07, Loss: 0.004826477728784084
[2023-11-05 23:24:00,743] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:24:04,061] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 173/170984 [09:36<158:45:00,  3.35s/it]11/05/2023 23:24:04 - INFO - __main__ -   Step: 173, LR: 6.74529681255483e-07, Loss: 0.20793603360652924
  0%|                                                                                                                                                         | 174/170984 [09:39<158:23:28,  3.34s/it]11/05/2023 23:24:07 - INFO - __main__ -   Step: 174, LR: 6.784286967540698e-07, Loss: 0.644939661026001
[2023-11-05 23:24:07,381] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 175/170984 [09:42<157:48:08,  3.33s/it]11/05/2023 23:24:10 - INFO - __main__ -   Step: 175, LR: 6.823277122526563e-07, Loss: 0.03343038260936737
[2023-11-05 23:24:10,679] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:24:14,045] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 176/170984 [09:46<158:22:41,  3.34s/it]11/05/2023 23:24:14 - INFO - __main__ -   Step: 176, LR: 6.862267277512429e-07, Loss: 0.007730279117822647
  0%|                                                                                                                                                         | 177/170984 [09:49<157:43:30,  3.32s/it]11/05/2023 23:24:17 - INFO - __main__ -   Step: 177, LR: 6.901257432498295e-07, Loss: 0.0041330778039991856
[2023-11-05 23:24:17,338] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 178/170984 [09:52<157:22:59,  3.32s/it]11/05/2023 23:24:20 - INFO - __main__ -   Step: 178, LR: 6.940247587484161e-07, Loss: 0.018823320046067238
[2023-11-05 23:24:20,638] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 179/170984 [09:56<157:25:06,  3.32s/it]11/05/2023 23:24:23 - INFO - __main__ -   Step: 179, LR: 6.979237742470027e-07, Loss: 0.0011416273191571236
[2023-11-05 23:24:23,957] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 180/170984 [09:59<158:20:23,  3.34s/it]11/05/2023 23:24:27 - INFO - __main__ -   Step: 180, LR: 7.018227897455893e-07, Loss: 0.00022206403082236648
[2023-11-05 23:24:27,340] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 181/170984 [10:02<159:24:52,  3.36s/it]11/05/2023 23:24:30 - INFO - __main__ -   Step: 181, LR: 7.05721805244176e-07, Loss: 8.278815948870033e-05
[2023-11-05 23:24:30,753] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:24:34,113] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 182/170984 [10:06<159:25:07,  3.36s/it]11/05/2023 23:24:34 - INFO - __main__ -   Step: 182, LR: 7.096208207427625e-07, Loss: 0.5431008338928223
  0%|                                                                                                                                                         | 183/170984 [10:09<159:04:55,  3.35s/it]11/05/2023 23:24:37 - INFO - __main__ -   Step: 183, LR: 7.135198362413491e-07, Loss: 0.7197830677032471
[2023-11-05 23:24:37,450] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 184/170984 [10:12<157:38:23,  3.32s/it]11/05/2023 23:24:40 - INFO - __main__ -   Step: 184, LR: 7.174188517399357e-07, Loss: 0.00019545896793715656
[2023-11-05 23:24:40,702] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 185/170984 [10:16<159:12:50,  3.36s/it]11/05/2023 23:24:44 - INFO - __main__ -   Step: 185, LR: 7.213178672385224e-07, Loss: 0.00025078319595195353
[2023-11-05 23:24:44,135] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 186/170984 [10:19<158:29:39,  3.34s/it]11/05/2023 23:24:47 - INFO - __main__ -   Step: 186, LR: 7.252168827371089e-07, Loss: 0.3141929507255554
[2023-11-05 23:24:47,440] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 187/170984 [10:22<158:15:50,  3.34s/it]11/05/2023 23:24:50 - INFO - __main__ -   Step: 187, LR: 7.291158982356955e-07, Loss: 7.215012738015503e-05
[2023-11-05 23:24:50,765] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 188/170984 [10:26<157:56:24,  3.33s/it]11/05/2023 23:24:54 - INFO - __main__ -   Step: 188, LR: 7.330149137342822e-07, Loss: 0.46841299533843994
[2023-11-05 23:24:54,078] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 189/170984 [10:29<157:09:43,  3.31s/it]11/05/2023 23:24:57 - INFO - __main__ -   Step: 189, LR: 7.369139292328687e-07, Loss: 0.00010008020035456866
[2023-11-05 23:24:57,353] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 190/170984 [10:32<156:52:34,  3.31s/it]11/05/2023 23:25:00 - INFO - __main__ -   Step: 190, LR: 7.408129447314553e-07, Loss: 0.3588935434818268
[2023-11-05 23:25:00,645] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 191/170984 [10:36<157:45:25,  3.33s/it]11/05/2023 23:25:04 - INFO - __main__ -   Step: 191, LR: 7.447119602300419e-07, Loss: 0.17435705661773682
[2023-11-05 23:25:04,014] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 192/170984 [10:39<158:29:09,  3.34s/it]11/05/2023 23:25:07 - INFO - __main__ -   Step: 192, LR: 7.486109757286287e-07, Loss: 0.3339552879333496
[2023-11-05 23:25:07,390] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 193/170984 [10:42<158:35:13,  3.34s/it]11/05/2023 23:25:10 - INFO - __main__ -   Step: 193, LR: 7.525099912272152e-07, Loss: 0.0036883228458464146
[2023-11-05 23:25:10,738] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 194/170984 [10:46<158:47:06,  3.35s/it]11/05/2023 23:25:14 - INFO - __main__ -   Step: 194, LR: 7.564090067258017e-07, Loss: 0.03145014867186546
[2023-11-05 23:25:14,095] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 195/170984 [10:49<158:27:00,  3.34s/it]11/05/2023 23:25:17 - INFO - __main__ -   Step: 195, LR: 7.603080222243885e-07, Loss: 0.0035844554658979177
[2023-11-05 23:25:17,418] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 196/170984 [10:52<158:20:45,  3.34s/it]11/05/2023 23:25:20 - INFO - __main__ -   Step: 196, LR: 7.642070377229751e-07, Loss: 0.40619462728500366
[2023-11-05 23:25:20,751] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 197/170984 [10:56<160:11:04,  3.38s/it]11/05/2023 23:25:24 - INFO - __main__ -   Step: 197, LR: 7.681060532215616e-07, Loss: 0.48502036929130554
[2023-11-05 23:25:24,218] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 198/170984 [10:59<159:44:31,  3.37s/it]11/05/2023 23:25:27 - INFO - __main__ -   Step: 198, LR: 7.720050687201482e-07, Loss: 0.004693922586739063
[2023-11-05 23:25:27,563] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 199/170984 [11:03<158:49:18,  3.35s/it]11/05/2023 23:25:30 - INFO - __main__ -   Step: 199, LR: 7.759040842187349e-07, Loss: 0.0031710986513644457
[2023-11-05 23:25:30,866] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 200/170984 [11:06<158:33:30,  3.34s/it]11/05/2023 23:25:34 - INFO - __main__ -   Step: 200, LR: 7.798030997173214e-07, Loss: 0.0010367685463279486
[2023-11-05 23:25:34,195] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 201/170984 [11:09<158:35:44,  3.34s/it]11/05/2023 23:25:37 - INFO - __main__ -   Step: 201, LR: 7.83702115215908e-07, Loss: 0.0014703861670568585
[2023-11-05 23:25:37,540] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 202/170984 [11:13<158:50:20,  3.35s/it]11/05/2023 23:25:40 - INFO - __main__ -   Step: 202, LR: 7.876011307144947e-07, Loss: 0.0022743125446140766
[2023-11-05 23:25:40,901] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:25:44,157] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 203/170984 [11:16<157:32:04,  3.32s/it]11/05/2023 23:25:44 - INFO - __main__ -   Step: 203, LR: 7.915001462130813e-07, Loss: 0.000903862644918263
  0%|                                                                                                                                                         | 204/170984 [11:19<158:12:55,  3.34s/it]11/05/2023 23:25:47 - INFO - __main__ -   Step: 204, LR: 7.953991617116678e-07, Loss: 0.001556673552840948
[2023-11-05 23:25:47,526] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 205/170984 [11:22<157:26:07,  3.32s/it]11/05/2023 23:25:50 - INFO - __main__ -   Step: 205, LR: 7.992981772102545e-07, Loss: 0.00013537757331505418
[2023-11-05 23:25:50,806] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 206/170984 [11:26<157:16:06,  3.32s/it]11/05/2023 23:25:54 - INFO - __main__ -   Step: 206, LR: 8.031971927088411e-07, Loss: 0.33324167132377625
[2023-11-05 23:25:54,113] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 207/170984 [11:29<158:08:27,  3.33s/it]11/05/2023 23:25:57 - INFO - __main__ -   Step: 207, LR: 8.070962082074277e-07, Loss: 0.412622332572937
[2023-11-05 23:25:57,490] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 208/170984 [11:32<157:38:51,  3.32s/it]11/05/2023 23:26:00 - INFO - __main__ -   Step: 208, LR: 8.109952237060142e-07, Loss: 0.001417150953784585
[2023-11-05 23:26:00,789] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 209/170984 [11:36<157:42:27,  3.32s/it]11/05/2023 23:26:04 - INFO - __main__ -   Step: 209, LR: 8.148942392046009e-07, Loss: 5.872542169527151e-05
[2023-11-05 23:26:04,117] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 210/170984 [11:39<158:39:10,  3.34s/it]11/05/2023 23:26:07 - INFO - __main__ -   Step: 210, LR: 8.187932547031875e-07, Loss: 0.00011138686386402696
[2023-11-05 23:26:07,508] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 211/170984 [11:42<158:21:43,  3.34s/it]11/05/2023 23:26:10 - INFO - __main__ -   Step: 211, LR: 8.22692270201774e-07, Loss: 6.699693767586723e-05
[2023-11-05 23:26:10,832] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 212/170984 [11:46<159:51:42,  3.37s/it]11/05/2023 23:26:14 - INFO - __main__ -   Step: 212, LR: 8.265912857003608e-07, Loss: 9.952277468983084e-05
[2023-11-05 23:26:14,275] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 213/170984 [11:49<159:05:46,  3.35s/it]11/05/2023 23:26:17 - INFO - __main__ -   Step: 213, LR: 8.304903011989474e-07, Loss: 0.7109919786453247
[2023-11-05 23:26:17,592] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 214/170984 [11:53<158:46:10,  3.35s/it]11/05/2023 23:26:20 - INFO - __main__ -   Step: 214, LR: 8.34389316697534e-07, Loss: 9.839223639573902e-05
[2023-11-05 23:26:20,923] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 215/170984 [11:56<157:44:37,  3.33s/it]11/05/2023 23:26:24 - INFO - __main__ -   Step: 215, LR: 8.382883321961205e-07, Loss: 0.4806712567806244
[2023-11-05 23:26:24,198] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 216/170984 [11:59<158:28:30,  3.34s/it]11/05/2023 23:26:27 - INFO - __main__ -   Step: 216, LR: 8.421873476947072e-07, Loss: 0.41009554266929626
[2023-11-05 23:26:27,575] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 217/170984 [12:03<158:06:28,  3.33s/it]11/05/2023 23:26:30 - INFO - __main__ -   Step: 217, LR: 8.460863631932938e-07, Loss: 0.952948808670044
[2023-11-05 23:26:30,890] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 218/170984 [12:06<158:02:48,  3.33s/it]11/05/2023 23:26:34 - INFO - __main__ -   Step: 218, LR: 8.499853786918804e-07, Loss: 0.00015744633856229484
[2023-11-05 23:26:34,219] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 219/170984 [12:09<158:53:41,  3.35s/it]11/05/2023 23:26:37 - INFO - __main__ -   Step: 219, LR: 8.53884394190467e-07, Loss: 0.37526533007621765
[2023-11-05 23:26:37,610] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 220/170984 [12:13<157:56:51,  3.33s/it]11/05/2023 23:26:40 - INFO - __main__ -   Step: 220, LR: 8.577834096890536e-07, Loss: 0.0002325815730728209
[2023-11-05 23:26:40,893] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 221/170984 [12:16<158:27:19,  3.34s/it]11/05/2023 23:26:44 - INFO - __main__ -   Step: 221, LR: 8.616824251876402e-07, Loss: 0.3955819010734558
[2023-11-05 23:26:44,259] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 222/170984 [12:19<158:08:21,  3.33s/it]11/05/2023 23:26:47 - INFO - __main__ -   Step: 222, LR: 8.655814406862267e-07, Loss: 0.0034662848338484764
[2023-11-05 23:26:47,577] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 223/170984 [12:23<158:52:31,  3.35s/it]11/05/2023 23:26:50 - INFO - __main__ -   Step: 223, LR: 8.694804561848134e-07, Loss: 0.00282043544575572
[2023-11-05 23:26:50,963] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 224/170984 [12:26<158:16:24,  3.34s/it]11/05/2023 23:26:54 - INFO - __main__ -   Step: 224, LR: 8.733794716834e-07, Loss: 0.002299819141626358
[2023-11-05 23:26:54,270] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 225/170984 [12:29<157:08:19,  3.31s/it]11/05/2023 23:26:57 - INFO - __main__ -   Step: 225, LR: 8.772784871819866e-07, Loss: 0.00020316889276728034
[2023-11-05 23:26:57,527] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 226/170984 [12:33<158:00:56,  3.33s/it]11/05/2023 23:27:00 - INFO - __main__ -   Step: 226, LR: 8.811775026805732e-07, Loss: 0.00039891671622172
[2023-11-05 23:27:00,902] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 227/170984 [12:36<158:08:00,  3.33s/it]11/05/2023 23:27:04 - INFO - __main__ -   Step: 227, LR: 8.850765181791598e-07, Loss: 0.3146122992038727
[2023-11-05 23:27:04,242] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 228/170984 [12:39<157:21:01,  3.32s/it]11/05/2023 23:27:07 - INFO - __main__ -   Step: 228, LR: 8.889755336777464e-07, Loss: 0.34391286969184875
[2023-11-05 23:27:07,521] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 229/170984 [12:42<157:06:30,  3.31s/it]11/05/2023 23:27:10 - INFO - __main__ -   Step: 229, LR: 8.92874549176333e-07, Loss: 0.7018835544586182
[2023-11-05 23:27:10,821] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 230/170984 [12:46<158:36:50,  3.34s/it]11/05/2023 23:27:14 - INFO - __main__ -   Step: 230, LR: 8.967735646749197e-07, Loss: 0.14033085107803345
[2023-11-05 23:27:14,239] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 231/170984 [12:49<158:55:20,  3.35s/it]11/05/2023 23:27:17 - INFO - __main__ -   Step: 231, LR: 9.006725801735063e-07, Loss: 0.2831655442714691
[2023-11-05 23:27:17,605] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 232/170984 [12:53<158:51:27,  3.35s/it]11/05/2023 23:27:20 - INFO - __main__ -   Step: 232, LR: 9.045715956720929e-07, Loss: 0.41444146633148193
[2023-11-05 23:27:20,951] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 233/170984 [12:56<158:21:20,  3.34s/it]11/05/2023 23:27:24 - INFO - __main__ -   Step: 233, LR: 9.084706111706795e-07, Loss: 0.18901297450065613
[2023-11-05 23:27:24,265] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 234/170984 [12:59<158:17:43,  3.34s/it]11/05/2023 23:27:27 - INFO - __main__ -   Step: 234, LR: 9.123696266692661e-07, Loss: 0.03755561262369156
[2023-11-05 23:27:27,599] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 235/170984 [13:03<158:12:52,  3.34s/it]11/05/2023 23:27:30 - INFO - __main__ -   Step: 235, LR: 9.162686421678527e-07, Loss: 0.0466444194316864
[2023-11-05 23:27:30,931] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 236/170984 [13:06<159:49:43,  3.37s/it]11/05/2023 23:27:34 - INFO - __main__ -   Step: 236, LR: 9.201676576664394e-07, Loss: 0.1732810139656067
[2023-11-05 23:27:34,380] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 237/170984 [13:09<159:02:13,  3.35s/it]11/05/2023 23:27:37 - INFO - __main__ -   Step: 237, LR: 9.240666731650259e-07, Loss: 0.11346320062875748
[2023-11-05 23:27:37,695] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 238/170984 [13:13<159:40:59,  3.37s/it]11/05/2023 23:27:41 - INFO - __main__ -   Step: 238, LR: 9.279656886636125e-07, Loss: 0.3612689673900604
[2023-11-05 23:27:41,093] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 239/170984 [13:16<159:28:05,  3.36s/it]11/05/2023 23:27:44 - INFO - __main__ -   Step: 239, LR: 9.318647041621991e-07, Loss: 0.42670005559921265
[2023-11-05 23:27:44,445] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 240/170984 [13:19<158:49:45,  3.35s/it]11/05/2023 23:27:47 - INFO - __main__ -   Step: 240, LR: 9.357637196607858e-07, Loss: 0.046267952769994736
[2023-11-05 23:27:47,762] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:27:51,163] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 241/170984 [13:23<159:33:58,  3.36s/it]11/05/2023 23:27:51 - INFO - __main__ -   Step: 241, LR: 9.396627351593723e-07, Loss: 0.07049201428890228
  0%|                                                                                                                                                         | 242/170984 [13:26<159:01:45,  3.35s/it]11/05/2023 23:27:54 - INFO - __main__ -   Step: 242, LR: 9.435617506579589e-07, Loss: 0.3261740803718567
[2023-11-05 23:27:54,490] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 243/170984 [13:29<158:10:38,  3.34s/it]11/05/2023 23:27:57 - INFO - __main__ -   Step: 243, LR: 9.474607661565456e-07, Loss: 0.08060482889413834
[2023-11-05 23:27:57,783] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:01,099] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 244/170984 [13:33<157:54:06,  3.33s/it]11/05/2023 23:28:01 - INFO - __main__ -   Step: 244, LR: 9.513597816551321e-07, Loss: 0.11890734732151031
  0%|                                                                                                                                                         | 245/170984 [13:36<157:39:31,  3.32s/it]11/05/2023 23:28:04 - INFO - __main__ -   Step: 245, LR: 9.552587971537188e-07, Loss: 0.002178346971049905
[2023-11-05 23:28:04,411] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 246/170984 [13:39<158:27:58,  3.34s/it]11/05/2023 23:28:07 - INFO - __main__ -   Step: 246, LR: 9.591578126523053e-07, Loss: 0.018715834245085716
[2023-11-05 23:28:07,792] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:11,055] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 247/170984 [13:43<157:21:05,  3.32s/it]11/05/2023 23:28:11 - INFO - __main__ -   Step: 247, LR: 9.63056828150892e-07, Loss: 0.0022437195293605328
  0%|                                                                                                                                                         | 248/170984 [13:46<158:18:07,  3.34s/it]11/05/2023 23:28:14 - INFO - __main__ -   Step: 248, LR: 9.669558436494786e-07, Loss: 0.21415653824806213
[2023-11-05 23:28:14,440] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 249/170984 [13:49<158:56:14,  3.35s/it]11/05/2023 23:28:17 - INFO - __main__ -   Step: 249, LR: 9.70854859148065e-07, Loss: 0.24446043372154236
[2023-11-05 23:28:17,822] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:21,142] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 250/170984 [13:53<158:29:34,  3.34s/it]11/05/2023 23:28:21 - INFO - __main__ -   Step: 250, LR: 9.747538746466518e-07, Loss: 0.0012986873043701053
  0%|                                                                                                                                                         | 251/170984 [13:56<158:13:37,  3.34s/it]11/05/2023 23:28:24 - INFO - __main__ -   Step: 251, LR: 9.786528901452385e-07, Loss: 0.00026020576478913426
[2023-11-05 23:28:24,466] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 252/170984 [13:59<158:45:01,  3.35s/it]11/05/2023 23:28:27 - INFO - __main__ -   Step: 252, LR: 9.82551905643825e-07, Loss: 0.0005839179502800107
[2023-11-05 23:28:27,839] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:31,310] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 253/170984 [14:03<160:30:23,  3.38s/it]11/05/2023 23:28:31 - INFO - __main__ -   Step: 253, LR: 9.864509211424116e-07, Loss: 0.00025438834563829005
[2023-11-05 23:28:34,746] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 254/170984 [14:06<161:14:25,  3.40s/it]11/05/2023 23:28:34 - INFO - __main__ -   Step: 254, LR: 9.903499366409983e-07, Loss: 0.48644208908081055
  0%|                                                                                                                                                         | 255/170984 [14:10<160:50:16,  3.39s/it]11/05/2023 23:28:38 - INFO - __main__ -   Step: 255, LR: 9.942489521395847e-07, Loss: 0.00016577410860918462
[2023-11-05 23:28:38,116] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 256/170984 [14:13<160:49:55,  3.39s/it]11/05/2023 23:28:41 - INFO - __main__ -   Step: 256, LR: 9.981479676381714e-07, Loss: 0.0001397712912876159
[2023-11-05 23:28:41,508] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:44,791] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 257/170984 [14:16<159:17:18,  3.36s/it]11/05/2023 23:28:44 - INFO - __main__ -   Step: 257, LR: 1.002046983136758e-06, Loss: 0.000170265935594216
  0%|                                                                                                                                                         | 258/170984 [14:20<158:49:39,  3.35s/it]11/05/2023 23:28:48 - INFO - __main__ -   Step: 258, LR: 1.0059459986353448e-06, Loss: 0.41850975155830383
[2023-11-05 23:28:48,118] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 259/170984 [14:23<158:46:28,  3.35s/it]11/05/2023 23:28:51 - INFO - __main__ -   Step: 259, LR: 1.0098450141339312e-06, Loss: 0.4281172454357147
[2023-11-05 23:28:51,463] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:28:54,760] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 260/170984 [14:26<158:02:55,  3.33s/it]11/05/2023 23:28:54 - INFO - __main__ -   Step: 260, LR: 1.013744029632518e-06, Loss: 0.00029363526846282184
  0%|                                                                                                                                                         | 261/170984 [14:30<159:23:01,  3.36s/it]11/05/2023 23:28:58 - INFO - __main__ -   Step: 261, LR: 1.0176430451311046e-06, Loss: 0.00032386710518039763
[2023-11-05 23:28:58,186] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 262/170984 [14:33<158:21:27,  3.34s/it]11/05/2023 23:29:01 - INFO - __main__ -   Step: 262, LR: 1.021542060629691e-06, Loss: 0.3703664243221283
[2023-11-05 23:29:01,476] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:04,792] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 263/170984 [14:36<158:01:49,  3.33s/it]11/05/2023 23:29:04 - INFO - __main__ -   Step: 263, LR: 1.0254410761282777e-06, Loss: 0.00021641372586600482
  0%|                                                                                                                                                         | 264/170984 [14:40<158:14:22,  3.34s/it]11/05/2023 23:29:08 - INFO - __main__ -   Step: 264, LR: 1.0293400916268644e-06, Loss: 0.0003440073924139142
[2023-11-05 23:29:08,139] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 265/170984 [14:43<158:47:23,  3.35s/it]11/05/2023 23:29:11 - INFO - __main__ -   Step: 265, LR: 1.0332391071254509e-06, Loss: 0.7401226758956909
[2023-11-05 23:29:11,515] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:14,971] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 266/170984 [14:47<160:18:52,  3.38s/it]11/05/2023 23:29:14 - INFO - __main__ -   Step: 266, LR: 1.0371381226240375e-06, Loss: 0.18366652727127075
  0%|                                                                                                                                                         | 267/170984 [14:50<160:55:34,  3.39s/it]11/05/2023 23:29:18 - INFO - __main__ -   Step: 267, LR: 1.0410371381226242e-06, Loss: 0.0008123897714540362
[2023-11-05 23:29:18,394] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 268/170984 [14:53<159:19:57,  3.36s/it]11/05/2023 23:29:21 - INFO - __main__ -   Step: 268, LR: 1.0449361536212107e-06, Loss: 0.8911939859390259
[2023-11-05 23:29:21,676] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:25,035] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 269/170984 [14:57<159:19:16,  3.36s/it]11/05/2023 23:29:25 - INFO - __main__ -   Step: 269, LR: 1.0488351691197974e-06, Loss: 0.5732484459877014
  0%|                                                                                                                                                         | 270/170984 [15:00<159:40:33,  3.37s/it]11/05/2023 23:29:28 - INFO - __main__ -   Step: 270, LR: 1.0527341846183838e-06, Loss: 0.026681240648031235
[2023-11-05 23:29:28,420] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 271/170984 [15:03<159:11:51,  3.36s/it]11/05/2023 23:29:31 - INFO - __main__ -   Step: 271, LR: 1.0566332001169705e-06, Loss: 0.004393320064991713
[2023-11-05 23:29:31,754] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:35,104] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 272/170984 [15:07<159:06:13,  3.36s/it]11/05/2023 23:29:35 - INFO - __main__ -   Step: 272, LR: 1.0605322156155572e-06, Loss: 0.003658871864899993
  0%|                                                                                                                                                         | 273/170984 [15:10<159:18:51,  3.36s/it]11/05/2023 23:29:38 - INFO - __main__ -   Step: 273, LR: 1.0644312311141436e-06, Loss: 0.00923670083284378
[2023-11-05 23:29:38,474] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 274/170984 [15:13<158:45:15,  3.35s/it]11/05/2023 23:29:41 - INFO - __main__ -   Step: 274, LR: 1.0683302466127305e-06, Loss: 0.0007896191673353314
[2023-11-05 23:29:41,795] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:45,158] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 275/170984 [15:17<158:58:27,  3.35s/it]11/05/2023 23:29:45 - INFO - __main__ -   Step: 275, LR: 1.072229262111317e-06, Loss: 0.33391574025154114
  0%|                                                                                                                                                         | 276/170984 [15:20<159:23:53,  3.36s/it]11/05/2023 23:29:48 - INFO - __main__ -   Step: 276, LR: 1.0761282776099037e-06, Loss: 0.007061446085572243
[2023-11-05 23:29:48,540] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 277/170984 [15:24<161:21:38,  3.40s/it]11/05/2023 23:29:52 - INFO - __main__ -   Step: 277, LR: 1.0800272931084901e-06, Loss: 0.0607147291302681
[2023-11-05 23:29:52,039] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:29:55,463] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 278/170984 [15:27<161:38:50,  3.41s/it]11/05/2023 23:29:55 - INFO - __main__ -   Step: 278, LR: 1.0839263086070768e-06, Loss: 0.009950573556125164
  0%|                                                                                                                                                         | 279/170984 [15:30<161:23:10,  3.40s/it]11/05/2023 23:29:58 - INFO - __main__ -   Step: 279, LR: 1.0878253241056635e-06, Loss: 0.2130574733018875
[2023-11-05 23:29:58,854] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 280/170984 [15:34<160:41:42,  3.39s/it]11/05/2023 23:30:02 - INFO - __main__ -   Step: 280, LR: 1.09172433960425e-06, Loss: 0.1392413228750229
[2023-11-05 23:30:02,209] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:05,562] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 281/170984 [15:37<160:11:03,  3.38s/it]11/05/2023 23:30:05 - INFO - __main__ -   Step: 281, LR: 1.0956233551028366e-06, Loss: 0.2576339840888977
  0%|                                                                                                                                                         | 282/170984 [15:40<158:56:21,  3.35s/it]11/05/2023 23:30:08 - INFO - __main__ -   Step: 282, LR: 1.0995223706014233e-06, Loss: 0.0010881500784307718
[2023-11-05 23:30:08,852] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 283/170984 [15:44<159:10:36,  3.36s/it]11/05/2023 23:30:12 - INFO - __main__ -   Step: 283, LR: 1.1034213861000098e-06, Loss: 0.0019658466335386038
[2023-11-05 23:30:12,221] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:15,500] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 284/170984 [15:47<158:04:01,  3.33s/it]11/05/2023 23:30:15 - INFO - __main__ -   Step: 284, LR: 1.1073204015985965e-06, Loss: 0.3158697783946991
  0%|                                                                                                                                                         | 285/170984 [15:51<158:43:05,  3.35s/it]11/05/2023 23:30:18 - INFO - __main__ -   Step: 285, LR: 1.1112194170971831e-06, Loss: 0.0005294597358442843
[2023-11-05 23:30:18,880] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 286/170984 [15:54<158:57:48,  3.35s/it]11/05/2023 23:30:22 - INFO - __main__ -   Step: 286, LR: 1.1151184325957696e-06, Loss: 0.3085193336009979
[2023-11-05 23:30:22,244] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 287/170984 [15:57<159:52:54,  3.37s/it]11/05/2023 23:30:25 - INFO - __main__ -   Step: 287, LR: 1.1190174480943563e-06, Loss: 0.006840745452791452
[2023-11-05 23:30:25,661] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:29,071] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 288/170984 [16:01<160:24:38,  3.38s/it]11/05/2023 23:30:29 - INFO - __main__ -   Step: 288, LR: 1.122916463592943e-06, Loss: 0.0014009787701070309
  0%|                                                                                                                                                         | 289/170984 [16:04<160:01:38,  3.38s/it]11/05/2023 23:30:32 - INFO - __main__ -   Step: 289, LR: 1.1268154790915294e-06, Loss: 0.4361112415790558
[2023-11-05 23:30:32,427] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 290/170984 [16:07<159:30:36,  3.36s/it]11/05/2023 23:30:35 - INFO - __main__ -   Step: 290, LR: 1.130714494590116e-06, Loss: 0.00030585264903493226
[2023-11-05 23:30:35,766] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:39,113] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 291/170984 [16:11<159:16:50,  3.36s/it]11/05/2023 23:30:39 - INFO - __main__ -   Step: 291, LR: 1.1346135100887026e-06, Loss: 0.0009841457940638065
  0%|                                                                                                                                                         | 292/170984 [16:14<160:28:37,  3.38s/it]11/05/2023 23:30:42 - INFO - __main__ -   Step: 292, LR: 1.1385125255872892e-06, Loss: 0.004552862141281366
[2023-11-05 23:30:42,557] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 293/170984 [16:18<159:48:15,  3.37s/it]11/05/2023 23:30:45 - INFO - __main__ -   Step: 293, LR: 1.142411541085876e-06, Loss: 0.23620474338531494
[2023-11-05 23:30:45,894] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:49,249] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 294/170984 [16:21<159:35:03,  3.37s/it]11/05/2023 23:30:49 - INFO - __main__ -   Step: 294, LR: 1.1463105565844624e-06, Loss: 0.0005073293577879667
  0%|                                                                                                                                                         | 295/170984 [16:24<159:02:21,  3.35s/it]11/05/2023 23:30:52 - INFO - __main__ -   Step: 295, LR: 1.1502095720830493e-06, Loss: 0.4333198368549347
[2023-11-05 23:30:52,576] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 296/170984 [16:28<159:25:40,  3.36s/it]11/05/2023 23:30:55 - INFO - __main__ -   Step: 296, LR: 1.1541085875816357e-06, Loss: 0.0022556325420737267
[2023-11-05 23:30:55,958] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:30:59,274] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 297/170984 [16:31<158:45:15,  3.35s/it]11/05/2023 23:30:59 - INFO - __main__ -   Step: 297, LR: 1.1580076030802224e-06, Loss: 0.46121060848236084
  0%|                                                                                                                                                         | 298/170984 [16:34<158:24:54,  3.34s/it]11/05/2023 23:31:02 - INFO - __main__ -   Step: 298, LR: 1.161906618578809e-06, Loss: 0.4747051000595093
[2023-11-05 23:31:02,598] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 299/170984 [16:38<158:37:03,  3.35s/it]11/05/2023 23:31:05 - INFO - __main__ -   Step: 299, LR: 1.1658056340773955e-06, Loss: 0.8108272552490234
[2023-11-05 23:31:05,954] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:09,279] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 300/170984 [16:41<158:20:06,  3.34s/it]11/05/2023 23:31:09 - INFO - __main__ -   Step: 300, LR: 1.1697046495759822e-06, Loss: 0.2854243814945221
  0%|                                                                                                                                                         | 301/170984 [16:44<157:26:11,  3.32s/it]11/05/2023 23:31:12 - INFO - __main__ -   Step: 301, LR: 1.1736036650745687e-06, Loss: 0.0014879193622618914
[2023-11-05 23:31:12,556] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 302/170984 [16:47<157:02:07,  3.31s/it]11/05/2023 23:31:15 - INFO - __main__ -   Step: 302, LR: 1.1775026805731554e-06, Loss: 0.07045163959264755
[2023-11-05 23:31:15,848] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:19,139] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 303/170984 [16:51<156:43:24,  3.31s/it]11/05/2023 23:31:19 - INFO - __main__ -   Step: 303, LR: 1.181401696071742e-06, Loss: 0.265880823135376
  0%|                                                                                                                                                         | 304/170984 [16:54<158:20:04,  3.34s/it]11/05/2023 23:31:22 - INFO - __main__ -   Step: 304, LR: 1.1853007115703285e-06, Loss: 0.043839260935783386
[2023-11-05 23:31:22,558] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 305/170984 [16:58<158:04:57,  3.33s/it]11/05/2023 23:31:25 - INFO - __main__ -   Step: 305, LR: 1.1891997270689152e-06, Loss: 0.11351160705089569
[2023-11-05 23:31:25,880] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:29,179] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 306/170984 [17:01<157:35:03,  3.32s/it]11/05/2023 23:31:29 - INFO - __main__ -   Step: 306, LR: 1.1930987425675019e-06, Loss: 0.03521639108657837
  0%|                                                                                                                                                         | 307/170984 [17:04<158:34:22,  3.34s/it]11/05/2023 23:31:32 - INFO - __main__ -   Step: 307, LR: 1.1969977580660883e-06, Loss: 0.6591980457305908
[2023-11-05 23:31:32,572] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 308/170984 [17:07<157:40:51,  3.33s/it]11/05/2023 23:31:35 - INFO - __main__ -   Step: 308, LR: 1.200896773564675e-06, Loss: 0.07464036345481873
[2023-11-05 23:31:35,854] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:39,238] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 309/170984 [17:11<158:30:02,  3.34s/it]11/05/2023 23:31:39 - INFO - __main__ -   Step: 309, LR: 1.2047957890632617e-06, Loss: 0.4120820164680481
  0%|                                                                                                                                                         | 310/170984 [17:14<157:53:06,  3.33s/it]11/05/2023 23:31:42 - INFO - __main__ -   Step: 310, LR: 1.2086948045618481e-06, Loss: 0.05832338333129883
[2023-11-05 23:31:42,538] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 311/170984 [17:18<157:49:03,  3.33s/it]11/05/2023 23:31:45 - INFO - __main__ -   Step: 311, LR: 1.2125938200604348e-06, Loss: 0.05440939962863922
[2023-11-05 23:31:45,864] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:49,139] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 312/170984 [17:21<157:03:12,  3.31s/it]11/05/2023 23:31:49 - INFO - __main__ -   Step: 312, LR: 1.2164928355590215e-06, Loss: 0.2081579864025116
  0%|                                                                                                                                                         | 313/170984 [17:24<157:09:35,  3.32s/it]11/05/2023 23:31:52 - INFO - __main__ -   Step: 313, LR: 1.2203918510576082e-06, Loss: 0.006878664717078209
[2023-11-05 23:31:52,458] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:31:55,763] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 314/170984 [17:27<157:00:18,  3.31s/it]11/05/2023 23:31:55 - INFO - __main__ -   Step: 314, LR: 1.2242908665561946e-06, Loss: 0.2761397957801819
  0%|                                                                                                                                                         | 315/170984 [17:31<157:09:41,  3.32s/it]11/05/2023 23:31:59 - INFO - __main__ -   Step: 315, LR: 1.2281898820547813e-06, Loss: 0.03338131308555603
[2023-11-05 23:31:59,086] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 316/170984 [17:34<157:19:21,  3.32s/it]11/05/2023 23:32:02 - INFO - __main__ -   Step: 316, LR: 1.232088897553368e-06, Loss: 0.4407205879688263
[2023-11-05 23:32:02,412] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:05,755] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 317/170984 [17:37<157:39:59,  3.33s/it]11/05/2023 23:32:05 - INFO - __main__ -   Step: 317, LR: 1.2359879130519545e-06, Loss: 0.005173464305698872
  0%|                                                                                                                                                         | 318/170984 [17:41<157:16:24,  3.32s/it]11/05/2023 23:32:09 - INFO - __main__ -   Step: 318, LR: 1.2398869285505411e-06, Loss: 0.0032982616685330868
[2023-11-05 23:32:09,053] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 319/170984 [17:44<157:12:40,  3.32s/it]11/05/2023 23:32:12 - INFO - __main__ -   Step: 319, LR: 1.2437859440491278e-06, Loss: 0.27757778763771057
[2023-11-05 23:32:12,366] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:15,734] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 320/170984 [17:47<157:55:53,  3.33s/it]11/05/2023 23:32:15 - INFO - __main__ -   Step: 320, LR: 1.2476849595477143e-06, Loss: 0.642653226852417
  0%|                                                                                                                                                         | 321/170984 [17:51<159:16:10,  3.36s/it]11/05/2023 23:32:19 - INFO - __main__ -   Step: 321, LR: 1.251583975046301e-06, Loss: 0.0005538092227652669
[2023-11-05 23:32:19,159] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 322/170984 [17:54<160:08:48,  3.38s/it]11/05/2023 23:32:22 - INFO - __main__ -   Step: 322, LR: 1.2554829905448876e-06, Loss: 0.000505386502481997
[2023-11-05 23:32:22,581] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:25,830] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 323/170984 [17:57<158:18:35,  3.34s/it]11/05/2023 23:32:25 - INFO - __main__ -   Step: 323, LR: 1.259382006043474e-06, Loss: 0.15565800666809082
  0%|                                                                                                                                                         | 324/170984 [18:01<156:11:33,  3.29s/it]11/05/2023 23:32:29 - INFO - __main__ -   Step: 324, LR: 1.2632810215420608e-06, Loss: 0.0040952181443572044
[2023-11-05 23:32:29,020] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 325/170984 [18:04<157:59:18,  3.33s/it]11/05/2023 23:32:32 - INFO - __main__ -   Step: 325, LR: 1.2671800370406472e-06, Loss: 0.2667023539543152
[2023-11-05 23:32:32,441] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:35,767] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 326/170984 [18:07<157:53:44,  3.33s/it]11/05/2023 23:32:35 - INFO - __main__ -   Step: 326, LR: 1.271079052539234e-06, Loss: 0.0007255970849655569
  0%|                                                                                                                                                         | 327/170984 [18:11<157:01:09,  3.31s/it]11/05/2023 23:32:39 - INFO - __main__ -   Step: 327, LR: 1.2749780680378204e-06, Loss: 0.2577022612094879
[2023-11-05 23:32:39,037] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 328/170984 [18:14<157:02:59,  3.31s/it]11/05/2023 23:32:42 - INFO - __main__ -   Step: 328, LR: 1.2788770835364073e-06, Loss: 0.0030879343394190073
[2023-11-05 23:32:42,351] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:45,708] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 329/170984 [18:17<157:39:50,  3.33s/it]11/05/2023 23:32:45 - INFO - __main__ -   Step: 329, LR: 1.282776099034994e-06, Loss: 0.0008789536077529192
  0%|                                                                                                                                                         | 330/170984 [18:21<157:35:01,  3.32s/it]11/05/2023 23:32:49 - INFO - __main__ -   Step: 330, LR: 1.2866751145335804e-06, Loss: 0.0005449797608889639
[2023-11-05 23:32:49,028] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 331/170984 [18:24<157:26:07,  3.32s/it]11/05/2023 23:32:52 - INFO - __main__ -   Step: 331, LR: 1.290574130032167e-06, Loss: 0.30180978775024414
[2023-11-05 23:32:52,342] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:32:55,632] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 332/170984 [18:27<156:59:10,  3.31s/it]11/05/2023 23:32:55 - INFO - __main__ -   Step: 332, LR: 1.2944731455307535e-06, Loss: 0.0015916903503239155
  0%|                                                                                                                                                         | 333/170984 [18:31<156:56:22,  3.31s/it]11/05/2023 23:32:58 - INFO - __main__ -   Step: 333, LR: 1.29837216102934e-06, Loss: 0.0008274454739876091
[2023-11-05 23:32:58,940] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 334/170984 [18:34<157:08:47,  3.32s/it]11/05/2023 23:33:02 - INFO - __main__ -   Step: 334, LR: 1.3022711765279267e-06, Loss: 0.3168184161186218
[2023-11-05 23:33:02,265] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:05,612] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 335/170984 [18:37<157:35:59,  3.32s/it]11/05/2023 23:33:05 - INFO - __main__ -   Step: 335, LR: 1.3061701920265136e-06, Loss: 0.1445014923810959
  0%|                                                                                                                                                         | 336/170984 [18:41<157:24:15,  3.32s/it]11/05/2023 23:33:08 - INFO - __main__ -   Step: 336, LR: 1.3100692075251e-06, Loss: 0.0003222024824935943
[2023-11-05 23:33:08,924] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 337/170984 [18:44<157:14:53,  3.32s/it]11/05/2023 23:33:12 - INFO - __main__ -   Step: 337, LR: 1.3139682230236867e-06, Loss: 0.0003785901644732803
[2023-11-05 23:33:12,233] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:15,560] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 338/170984 [18:47<157:22:50,  3.32s/it]11/05/2023 23:33:15 - INFO - __main__ -   Step: 338, LR: 1.3178672385222732e-06, Loss: 0.3650781214237213
  0%|                                                                                                                                                         | 339/170984 [18:51<157:40:16,  3.33s/it]11/05/2023 23:33:18 - INFO - __main__ -   Step: 339, LR: 1.3217662540208599e-06, Loss: 0.17072731256484985
[2023-11-05 23:33:18,901] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 340/170984 [18:54<158:03:35,  3.33s/it]11/05/2023 23:33:22 - INFO - __main__ -   Step: 340, LR: 1.3256652695194463e-06, Loss: 0.24700145423412323
[2023-11-05 23:33:22,254] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:25,622] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 341/170984 [18:57<158:31:47,  3.34s/it]11/05/2023 23:33:25 - INFO - __main__ -   Step: 341, LR: 1.329564285018033e-06, Loss: 0.0044449614360928535
  0%|                                                                                                                                                         | 342/170984 [19:01<158:17:04,  3.34s/it]11/05/2023 23:33:28 - INFO - __main__ -   Step: 342, LR: 1.3334633005166197e-06, Loss: 0.10757019370794296
[2023-11-05 23:33:28,949] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 343/170984 [19:04<157:26:12,  3.32s/it]11/05/2023 23:33:32 - INFO - __main__ -   Step: 343, LR: 1.3373623160152063e-06, Loss: 0.00608253525570035
[2023-11-05 23:33:32,229] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:35,579] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 344/170984 [19:07<157:50:10,  3.33s/it]11/05/2023 23:33:35 - INFO - __main__ -   Step: 344, LR: 1.3412613315137928e-06, Loss: 0.02298005111515522
  0%|                                                                                                                                                         | 345/170984 [19:10<156:38:22,  3.30s/it]11/05/2023 23:33:38 - INFO - __main__ -   Step: 345, LR: 1.3451603470123795e-06, Loss: 0.23177944123744965
[2023-11-05 23:33:38,824] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 346/170984 [19:14<157:09:34,  3.32s/it]11/05/2023 23:33:42 - INFO - __main__ -   Step: 346, LR: 1.349059362510966e-06, Loss: 0.010477685369551182
[2023-11-05 23:33:42,166] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:45,442] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 347/170984 [19:17<156:36:05,  3.30s/it]11/05/2023 23:33:45 - INFO - __main__ -   Step: 347, LR: 1.3529583780095526e-06, Loss: 0.05611805245280266
  0%|                                                                                                                                                         | 348/170984 [19:20<156:57:37,  3.31s/it]11/05/2023 23:33:48 - INFO - __main__ -   Step: 348, LR: 1.3568573935081395e-06, Loss: 0.005887559615075588
[2023-11-05 23:33:48,771] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:33:52,073] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 349/170984 [19:24<156:49:25,  3.31s/it]11/05/2023 23:33:52 - INFO - __main__ -   Step: 349, LR: 1.360756409006726e-06, Loss: 0.0003148026007693261
  0%|                                                                                                                                                         | 350/170984 [19:27<156:53:34,  3.31s/it]11/05/2023 23:33:55 - INFO - __main__ -   Step: 350, LR: 1.3646554245053127e-06, Loss: 0.0005263186176307499
[2023-11-05 23:33:55,387] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 351/170984 [19:30<156:47:52,  3.31s/it]11/05/2023 23:33:58 - INFO - __main__ -   Step: 351, LR: 1.3685544400038991e-06, Loss: 0.0012604804942384362
[2023-11-05 23:33:58,690] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:02,042] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 352/170984 [19:34<157:24:50,  3.32s/it]11/05/2023 23:34:02 - INFO - __main__ -   Step: 352, LR: 1.3724534555024858e-06, Loss: 6.399204721674323e-05
  0%|                                                                                                                                                         | 353/170984 [19:37<157:17:20,  3.32s/it]11/05/2023 23:34:05 - INFO - __main__ -   Step: 353, LR: 1.3763524710010723e-06, Loss: 0.26195332407951355
[2023-11-05 23:34:05,354] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 354/170984 [19:40<157:22:59,  3.32s/it]11/05/2023 23:34:08 - INFO - __main__ -   Step: 354, LR: 1.380251486499659e-06, Loss: 0.28047147393226624
[2023-11-05 23:34:08,679] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:12,023] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 355/170984 [19:44<157:42:13,  3.33s/it]11/05/2023 23:34:12 - INFO - __main__ -   Step: 355, LR: 1.3841505019982456e-06, Loss: 9.540874452795833e-05
  0%|                                                                                                                                                         | 356/170984 [19:47<156:55:55,  3.31s/it]11/05/2023 23:34:15 - INFO - __main__ -   Step: 356, LR: 1.3880495174968323e-06, Loss: 0.00010034936713054776
[2023-11-05 23:34:15,296] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 357/170984 [19:50<158:26:34,  3.34s/it]11/05/2023 23:34:18 - INFO - __main__ -   Step: 357, LR: 1.3919485329954188e-06, Loss: 0.32618477940559387
[2023-11-05 23:34:18,713] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:21,986] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 358/170984 [19:54<157:26:55,  3.32s/it]11/05/2023 23:34:21 - INFO - __main__ -   Step: 358, LR: 1.3958475484940054e-06, Loss: 8.32419318612665e-05
  0%|                                                                                                                                                         | 359/170984 [19:57<156:28:06,  3.30s/it]11/05/2023 23:34:25 - INFO - __main__ -   Step: 359, LR: 1.399746563992592e-06, Loss: 0.7131306529045105
[2023-11-05 23:34:25,239] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 360/170984 [20:00<156:43:49,  3.31s/it]11/05/2023 23:34:28 - INFO - __main__ -   Step: 360, LR: 1.4036455794911786e-06, Loss: 2.77673862001393e-05
[2023-11-05 23:34:28,559] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:31,932] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 361/170984 [20:04<157:40:22,  3.33s/it]11/05/2023 23:34:31 - INFO - __main__ -   Step: 361, LR: 1.407544594989765e-06, Loss: 8.478814561385661e-05
  0%|                                                                                                                                                         | 362/170984 [20:07<157:40:25,  3.33s/it]11/05/2023 23:34:35 - INFO - __main__ -   Step: 362, LR: 1.411443610488352e-06, Loss: 0.2358735352754593
[2023-11-05 23:34:35,259] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 363/170984 [20:10<157:28:37,  3.32s/it]11/05/2023 23:34:38 - INFO - __main__ -   Step: 363, LR: 1.4153426259869384e-06, Loss: 0.29941326379776
[2023-11-05 23:34:38,572] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:41,882] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 364/170984 [20:14<157:17:32,  3.32s/it]11/05/2023 23:34:41 - INFO - __main__ -   Step: 364, LR: 1.419241641485525e-06, Loss: 0.0026787847746163607
  0%|                                                                                                                                                         | 365/170984 [20:17<158:13:46,  3.34s/it]11/05/2023 23:34:45 - INFO - __main__ -   Step: 365, LR: 1.4231406569841115e-06, Loss: 0.0007845615036785603
[2023-11-05 23:34:45,267] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 366/170984 [20:20<157:57:59,  3.33s/it]11/05/2023 23:34:48 - INFO - __main__ -   Step: 366, LR: 1.4270396724826982e-06, Loss: 0.0034967316314578056
[2023-11-05 23:34:48,587] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:34:51,871] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 367/170984 [20:24<157:16:20,  3.32s/it]11/05/2023 23:34:51 - INFO - __main__ -   Step: 367, LR: 1.4309386879812847e-06, Loss: 0.0005631062667816877
  0%|                                                                                                                                                         | 368/170984 [20:27<158:08:57,  3.34s/it]11/05/2023 23:34:55 - INFO - __main__ -   Step: 368, LR: 1.4348377034798714e-06, Loss: 0.4082156717777252
[2023-11-05 23:34:55,251] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 369/170984 [20:30<157:47:00,  3.33s/it]11/05/2023 23:34:58 - INFO - __main__ -   Step: 369, LR: 1.4387367189784582e-06, Loss: 0.01032357756048441
[2023-11-05 23:34:58,563] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:01,906] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 370/170984 [20:34<157:59:23,  3.33s/it]11/05/2023 23:35:01 - INFO - __main__ -   Step: 370, LR: 1.4426357344770447e-06, Loss: 0.16993176937103271
  0%|                                                                                                                                                         | 371/170984 [20:37<157:55:58,  3.33s/it]11/05/2023 23:35:05 - INFO - __main__ -   Step: 371, LR: 1.4465347499756314e-06, Loss: 0.30153197050094604
[2023-11-05 23:35:05,236] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 372/170984 [20:40<159:28:26,  3.36s/it]11/05/2023 23:35:08 - INFO - __main__ -   Step: 372, LR: 1.4504337654742179e-06, Loss: 0.0013339387951418757
[2023-11-05 23:35:08,677] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:12,038] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 373/170984 [20:44<159:25:04,  3.36s/it]11/05/2023 23:35:12 - INFO - __main__ -   Step: 373, LR: 1.4543327809728045e-06, Loss: 0.4730132818222046
  0%|                                                                                                                                                         | 374/170984 [20:47<158:36:05,  3.35s/it]11/05/2023 23:35:15 - INFO - __main__ -   Step: 374, LR: 1.458231796471391e-06, Loss: 0.14764681458473206
[2023-11-05 23:35:15,344] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 375/170984 [20:50<158:40:03,  3.35s/it]11/05/2023 23:35:18 - INFO - __main__ -   Step: 375, LR: 1.4621308119699777e-06, Loss: 0.004051064141094685
[2023-11-05 23:35:18,696] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:22,017] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 376/170984 [20:54<158:16:57,  3.34s/it]11/05/2023 23:35:22 - INFO - __main__ -   Step: 376, LR: 1.4660298274685643e-06, Loss: 0.03714902326464653
  0%|                                                                                                                                                         | 377/170984 [20:57<157:59:46,  3.33s/it]11/05/2023 23:35:25 - INFO - __main__ -   Step: 377, LR: 1.469928842967151e-06, Loss: 0.16228796541690826
[2023-11-05 23:35:25,337] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:28,731] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 378/170984 [21:00<158:51:34,  3.35s/it]11/05/2023 23:35:28 - INFO - __main__ -   Step: 378, LR: 1.4738278584657375e-06, Loss: 0.015368576161563396
[2023-11-05 23:35:32,077] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 379/170984 [21:04<158:46:20,  3.35s/it]11/05/2023 23:35:32 - INFO - __main__ -   Step: 379, LR: 1.4777268739643242e-06, Loss: 0.05439214035868645
  0%|                                                                                                                                                         | 380/170984 [21:07<158:23:29,  3.34s/it]11/05/2023 23:35:35 - INFO - __main__ -   Step: 380, LR: 1.4816258894629106e-06, Loss: 0.0027472847141325474
[2023-11-05 23:35:35,401] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:38,706] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 381/170984 [21:10<157:51:53,  3.33s/it]11/05/2023 23:35:38 - INFO - __main__ -   Step: 381, LR: 1.4855249049614973e-06, Loss: 0.34450003504753113
[2023-11-05 23:35:42,091] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 382/170984 [21:14<158:37:13,  3.35s/it]11/05/2023 23:35:42 - INFO - __main__ -   Step: 382, LR: 1.4894239204600838e-06, Loss: 0.0067544179037213326
  0%|                                                                                                                                                         | 383/170984 [21:17<158:27:15,  3.34s/it]11/05/2023 23:35:45 - INFO - __main__ -   Step: 383, LR: 1.4933229359586707e-06, Loss: 0.1492539942264557
[2023-11-05 23:35:45,426] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:48,769] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 384/170984 [21:20<158:26:36,  3.34s/it]11/05/2023 23:35:48 - INFO - __main__ -   Step: 384, LR: 1.4972219514572573e-06, Loss: 0.001183300861157477
[2023-11-05 23:35:52,048] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 385/170984 [21:24<157:31:08,  3.32s/it]11/05/2023 23:35:52 - INFO - __main__ -   Step: 385, LR: 1.5011209669558438e-06, Loss: 0.00034404979669488966
  0%|                                                                                                                                                         | 386/170984 [21:27<157:33:52,  3.32s/it]11/05/2023 23:35:55 - INFO - __main__ -   Step: 386, LR: 1.5050199824544305e-06, Loss: 0.4967590272426605
[2023-11-05 23:35:55,375] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:35:58,697] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 387/170984 [21:30<157:31:35,  3.32s/it]11/05/2023 23:35:58 - INFO - __main__ -   Step: 387, LR: 1.508918997953017e-06, Loss: 0.0004421300836838782
[2023-11-05 23:36:02,060] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 388/170984 [21:34<158:04:15,  3.34s/it]11/05/2023 23:36:02 - INFO - __main__ -   Step: 388, LR: 1.5128180134516034e-06, Loss: 0.2874828279018402
  0%|                                                                                                                                                         | 389/170984 [21:37<161:36:59,  3.41s/it]11/05/2023 23:36:05 - INFO - __main__ -   Step: 389, LR: 1.51671702895019e-06, Loss: 0.365053653717041
[2023-11-05 23:36:05,645] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 390/170984 [21:41<160:36:30,  3.39s/it]11/05/2023 23:36:08 - INFO - __main__ -   Step: 390, LR: 1.520616044448777e-06, Loss: 0.002692903159186244
[2023-11-05 23:36:08,985] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:36:12,312] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 391/170984 [21:44<159:43:47,  3.37s/it]11/05/2023 23:36:12 - INFO - __main__ -   Step: 391, LR: 1.5245150599473634e-06, Loss: 0.3131660521030426
  0%|                                                                                                                                                         | 392/170984 [21:47<159:16:12,  3.36s/it]11/05/2023 23:36:15 - INFO - __main__ -   Step: 392, LR: 1.5284140754459501e-06, Loss: 0.0022938402835279703
[2023-11-05 23:36:15,651] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:36:18,987] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 393/170984 [21:51<158:54:33,  3.35s/it]11/05/2023 23:36:18 - INFO - __main__ -   Step: 393, LR: 1.5323130909445366e-06, Loss: 0.3515763580799103
[2023-11-05 23:36:22,228] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 394/170984 [21:54<157:18:56,  3.32s/it]11/05/2023 23:36:22 - INFO - __main__ -   Step: 394, LR: 1.5362121064431233e-06, Loss: 0.00094976945547387
  0%|                                                                                                                                                         | 395/170984 [21:57<157:09:40,  3.32s/it]11/05/2023 23:36:25 - INFO - __main__ -   Step: 395, LR: 1.5401111219417097e-06, Loss: 0.0042501636780798435
[2023-11-05 23:36:25,537] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:36:28,876] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 396/170984 [22:01<157:28:40,  3.32s/it]11/05/2023 23:36:28 - INFO - __main__ -   Step: 396, LR: 1.5440101374402964e-06, Loss: 0.0027792546898126602
[2023-11-05 23:36:32,220] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 397/170984 [22:04<157:46:21,  3.33s/it]11/05/2023 23:36:32 - INFO - __main__ -   Step: 397, LR: 1.547909152938883e-06, Loss: 0.0008673658594489098
  0%|                                                                                                                                                         | 398/170984 [22:07<156:40:19,  3.31s/it]11/05/2023 23:36:35 - INFO - __main__ -   Step: 398, LR: 1.5518081684374697e-06, Loss: 0.0016990938456729054
[2023-11-05 23:36:35,472] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:36:38,723] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 399/170984 [22:10<155:52:59,  3.29s/it]11/05/2023 23:36:38 - INFO - __main__ -   Step: 399, LR: 1.5557071839360562e-06, Loss: 0.5914028882980347
[2023-11-05 23:36:42,014] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 400/170984 [22:14<155:53:22,  3.29s/it]11/05/2023 23:36:42 - INFO - __main__ -   Step: 400, LR: 1.5596061994346429e-06, Loss: 7.51279731048271e-05
11/05/2023 23:36:42 - INFO - __main__ - Start doing evaluation at step: 400
 14%|                                                                                                                                             | 1/7 [00:01<00:07,  1.29s/it]
torch.Size([32, 176, 32004]) torch.Size([32, 176])
 29%|                                                                                                                     | 2/7 [00:02<00:06,  1.21s/it]
 29%|                                                                                                                     | 2/7 [00:02<00:06,  1.21s/it]
torch.Size([32, 128, 32004]) torch.Size([32, 128])
 57%|                                                                      | 4/7 [00:04<00:03,  1.20s/it]
 57%|                                                                      | 4/7 [00:04<00:03,  1.20s/it]
 86%|                       | 6/7 [00:07<00:01,  1.15s/it]
 86%|                       | 6/7 [00:07<00:01,  1.15s/it]
torch.Size([32, 126, 32004]) torch.Size([32, 126])
torch.Size([32, 127, 32004]) torch.Size([32, 127])
Pred: 30488, GT: 16387
Pred: , GT: realized
Pred: 30488, GT: 10943
Pred: , GT: discovered
Pred: 30488, GT: 10548
Pred: , GT: noticed
Pred: 30488, GT: 16387
Pred: , GT: realized
Pred: 30488, GT: 443
Pred: , GT: un
Pred: 30488, GT: 10943
Pred: , GT: discovered
Pred: 30488, GT: 14831
Pred: , GT: recognized
Pred: 30488, GT: 16387
Pred: , GT: realized
Pred: 30488, GT: 10943
Pred: , GT: discovered
Pred: 30488, GT: 10548
Pred: , GT: noticed
Pred: 30488, GT: 14831
Pred: , GT: recognized
Pred: 30488, GT: 17515
Pred: , GT: gained
Pred: 30488, GT: 11098
Pred: , GT: understood
Pred: 30488, GT: 16692
Pred: , GT: acquired
Pred: 30488, GT: 16387
Pred: , GT: realized
Pred: 30488, GT: 22229
Pred: , GT: gathered
Pred: 30488, GT: 10972
Pred: , GT: learned
Pred: 30488, GT: 10943
Pred: , GT: discovered
Pred: 30488, GT: 5835
Pred: , GT: master
Pred: 30488, GT: 8906
Pred: , GT: developed
Pred: 30488, GT: 9177
Pred: , GT: warning
Pred: 30488, GT: 9177
Pred: , GT: warning
Pred: 30488, GT: 29383
Pred: , GT: warn
Pred: 30488, GT: 1871
Pred: , GT: inform
Pred: 30488, GT: 9177
Pred: , GT: warning
Pred: 30488, GT: 9177
Pred: , GT: warning
Pred: 30488, GT: 17386
Pred: , GT: recall
Pred: 30488, GT: 5353
Pred: , GT: discuss
Pred: 30488, GT: 7182
Pred: , GT: signal
Pred: 30488, GT: 9177
Pred: , GT: warning
Pred: 1, GT: 2906
Pred: <s>, GT: dev
Pred: 30488, GT: 6635
Pred: , GT: cat
Pred: 32001, GT: 25305
Pred: <sep>, GT: trag
Pred: 32001, GT: 16403
Pred: <sep>, GT: terrible
Pred: 32001, GT: 2906
Pred: <sep>, GT: dev
Pred: 32001, GT: 766
Pred: <sep>, GT: dis
Pred: 32001, GT: 9358
Pred: <sep>, GT: ep
Pred: 30488, GT: 6635
Pred: , GT: cat
Pred: 1, GT: 2906
Pred: <s>, GT: dev
Pred: 29889, GT: 1020
Pred: ., GT: tra
Pred: 30488, GT: 7180
Pred: , GT: placed
Pred: 30488, GT: 1476
Pred: , GT: found
Pred: 1, GT: 639
Pred: <s>, GT: per
Pred: 1, GT: 14089
Pred: <s>, GT: park
Pred: 30488, GT: 8833
Pred: , GT: displayed
Pred: 30488, GT: 5685
Pred: , GT: flo
Pred: 30488, GT: 4240
Pred: , GT: built
Pred: 30488, GT: 7180
Pred: , GT: placed
Pred: 30488, GT: 10943
Pred: , GT: discovered
Pred: 30488, GT: 14737
Pred: , GT: ere
Pred: 30488, GT: 21050
Pred: , GT: arranged
Pred: 30488, GT: 2602
Pred: , GT: position
Pred: 30488, GT: 2602
Pred: , GT: position
Pred: 30488, GT: 4629
Pred: , GT: selected
Pred: 30488, GT: 21050
Pred: , GT: arranged
Pred: 30488, GT: 2602
Pred: , GT: position
Pred: 30488, GT: 7180
Pred: , GT: placed
Pred: 30488, GT: 8833
Pred: , GT: displayed
Pred: 30488, GT: 4629
Pred: , GT: selected
Pred: 30488, GT: 10624
Pred: , GT: directed
Pred: 30488, GT: 1476
Pred: , GT: found
Pred: 30488, GT: 17141
Pred: , GT: settled
Pred: 30488, GT: 2175
Pred: , GT: left
Pred: 30488, GT: 7180
Pred: , GT: placed
Pred: 32001, GT: 9698
Pred: <sep>, GT: ended
Pred: 32001, GT: 6153
Pred: <sep>, GT: moved
Pred: 32001, GT: 6087
Pred: <sep>, GT: stored
Pred: 32001, GT: 7500
Pred: <sep>, GT: loaded
Pred: 32001, GT: 620
Pred: <sep>, GT: res
Pred: 32001, GT: 21050
Pred: <sep>, GT: arranged
Pred: 32001, GT: 5330
Pred: <sep>, GT: ign
Pred: 32001, GT: 19799
Pred: <sep>, GT: triggered
Pred: 32001, GT: 16267
Pred: <sep>, GT: spark
Pred: 32001, GT: 14511
Pred: <sep>, GT: initi
Pred: 32001, GT: 5331
Pred: <sep>, GT: led
Pred: 32001, GT: 731
Pred: <sep>, GT: set
Pred: 32001, GT: 5330
Pred: <sep>, GT: ign
Pred: 32001, GT: 19799
Pred: <sep>, GT: triggered
Pred: 32001, GT: 14511
Pred: <sep>, GT: initi
Pred: 32001, GT: 16267
Pred: <sep>, GT: spark
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 12853
Pred: <sep>, GT: welcome
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 1395
Pred: <s>, GT: gre
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 5768
Pred: <s>, GT: drop
Pred: 1, GT: 16671
Pred: <s>, GT: surprise
Pred: 1, GT: 6493
Pred: <s>, GT: visit
Pred: 1, GT: 772
Pred: <s>, GT: po
Pred: 1, GT: 5146
Pred: <s>, GT: pay
Pred: 1, GT: 8459
Pred: <s>, GT: decided
Pred: 1, GT: 2714
Pred: <s>, GT: thought
Pred: 1, GT: 16671
Pred: <s>, GT: surprise
Pred: 1, GT: 772
Pred: <s>, GT: po
Pred: 1, GT: 18014
Pred: <s>, GT: surprised
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 18517
Pred: <s>, GT: arrival
Pred: 1, GT: 26834
Pred: <s>, GT: verified
Pred: 1, GT: 10018
Pred: <s>, GT: showed
Pred: 1, GT: 885
Pred: <s>, GT: sc
Pred: 1, GT: 9132
Pred: <s>, GT: presented
Pred: 1, GT: 7625
Pred: <s>, GT: obtained
Pred: 1, GT: 8794
Pred: <s>, GT: signed
Pred: 1, GT: 13877
Pred: <s>, GT: requested
Pred: 1, GT: 10018
Pred: <s>, GT: showed
Pred: 1, GT: 1304
Pred: <s>, GT: used
Pred: 1, GT: 4520
Pred: <s>, GT: received
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 1, GT: 11977
Pred: <s>, GT: arrived
Pred: 32001, GT: 11977
Pred: <sep>, GT: arrived
Pred: 32001, GT: 11977
Pred: <sep>, GT: arrived
Pred: 32001, GT: 6974
Pred: <sep>, GT: arriv
Pred: 32001, GT: 6974
Pred: <sep>, GT: arriv
Pred: 32001, GT: 18331
Pred: <sep>, GT: arrive
Pred: 32001, GT: 18331
Pred: <sep>, GT: arrive
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 18331
Pred: <sep>, GT: arrive
Pred: 32001, GT: 18331
Pred: <sep>, GT: arrive
Pred: 32001, GT: 18517
Pred: <sep>, GT: arrival
Pred: 32001, GT: 18331
Pred: <sep>, GT: arrive
Pred: 32001, GT: 20115
Pred: <sep>, GT: delivered
Pred: 32001, GT: 14523
Pred: <sep>, GT: exchange
Pred: 32001, GT: 27214
Pred: <sep>, GT: negoti
Pred: 1, GT: 4520
Pred: <s>, GT: received
Pred: 1, GT: 5839
Pred: <s>, GT: pick
Pred: 1, GT: 28289
Pred: <s>, GT: delivery
Pred: 1, GT: 7751
Pred: <s>, GT: ship
Pred: 1, GT: 11817
Pred: <s>, GT: invent
Pred: 1, GT: 2143
Pred: <s>, GT: ref
Pred: 1, GT: 10804
Pred: <s>, GT: transaction
Pred: 1, GT: 20115
Pred: <s>, GT: delivered
Pred: 1, GT: 23110
Pred: <s>, GT: tracking
Pred: 1, GT: 3234
Pred: <s>, GT: product
Pred: 1, GT: 9659
Pred: <s>, GT: confirm
Pred: 1, GT: 13271
Pred: <s>, GT: updating
Pred: 1, GT: 6909
Pred: <s>, GT: money
Pred: 1, GT: 9528
Pred: <s>, GT: tested
Pred: 1, GT: 28289
Pred: <s>, GT: delivery
Pred: 1, GT: 1361
Pred: <s>, GT: hand
Pred: 1, GT: 2894
Pred: <s>, GT: organ
Pred: 1, GT: 20115
Pred: <s>, GT: delivered
Pred: 1, GT: 8676
Pred: <s>, GT: completed
Pred: 1, GT: 4944
Pred: <s>, GT: provided
Pred: 1, GT: 13916
Pred: <s>, GT: dispatch
Pred: 1, GT: 9259
Pred: <s>, GT: accepted
Pred: 1, GT: 20115
Pred: <s>, GT: delivered
Pred: 1, GT: 29692
Pred: <s>, GT: handed
Pred: 1, GT: 21467
Pred: <s>, GT: scheduled
Pred: 1, GT: 1754
Pred: <s>, GT: made
Pred: 1, GT: 8676
Pred: <s>, GT: completed
Pred: 1, GT: 20115
Pred: <s>, GT: delivered
Pred: 1, GT: 28289
Pred: <s>, GT: delivery
Pred: 1, GT: 29692
Pred: <s>, GT: handed
Pred: 1, GT: 13916
Pred: <s>, GT: dispatch
Pred: 1, GT: 3271
Pred: <s>, GT: home
Pred: 1, GT: 1021
Pred: <s>, GT: same
Pred: 1, GT: 18440
Pred: <s>, GT: transferred
Pred: 1, GT: 20115
Pred: <s>, GT: delivered
Pred: 1, GT: 8608
Pred: <s>, GT: transport
Pred: 1, GT: 28289
Pred: <s>, GT: delivery
Pred: 1, GT: 5239
Pred: <s>, GT: sold
Pred: 1, GT: 1016
Pred: <s>, GT: don
Pred: 1, GT: 4502
Pred: <s>, GT: passed
Pred: 1, GT: 8794
Pred: <s>, GT: signed
Pred: 1, GT: 281
Pred: <s>, GT: w
Pred: 1, GT: 15074
Pred: <s>, GT: awarded
Pred: 1, GT: 16896
Pred: <s>, GT: granted
Pred: 1, GT: 5239
Pred: <s>, GT: sold
Pred: 1, GT: 20848
Pred: <s>, GT: purchased
Pred: 1, GT: 2665
Pred: <s>, GT: sent
Pred: 1, GT: 28289
Pred: <s>, GT: delivery
Pred: 1, GT: 6782
Pred: <s>, GT: transfer
Pred: 32001, GT: 28289
Pred: <sep>, GT: delivery
Pred: 32001, GT: 6782
Pred: <sep>, GT: transfer
Pred: 32001, GT: 4918
Pred: <sep>, GT: Post
Pred: 32001, GT: 6782
Pred: <sep>, GT: transfer
Pred: 32001, GT: 28289
Pred: <sep>, GT: delivery
Pred: 32001, GT: 6782
Pred: <sep>, GT: transfer
Pred: 32001, GT: 6782
Pred: <sep>, GT: transfer
Pred: 32001, GT: 28705
Pred: <sep>, GT: argued
Pred: 32001, GT: 9132
Pred: <sep>, GT: presented
Pred: 32001, GT: 10824
Pred: <sep>, GT: explained
Pred: 32001, GT: 2553
Pred: <sep>, GT: deb
Pred: 32001, GT: 29537
Pred: <sep>, GT: analyz
Pred: 32001, GT: 15648
Pred: <sep>, GT: discussed
Pred: 32001, GT: 9132
Pred: <sep>, GT: presented
Pred: 32001, GT: 29537
Pred: <sep>, GT: analyz
Pred: 32001, GT: 8575
Pred: <sep>, GT: march
Pred: 32001, GT: 22229
Pred: <sep>, GT: gathered
Pred: 32001, GT: 3512
Pred: <sep>, GT: went
Pred: 32001, GT: 19098
Pred: <sep>, GT: organized
Pred: 32001, GT: 24370
Pred: <sep>, GT: blocked
Pred: 32001, GT: 19098
Pred: <sep>, GT: organized
Pred: 32001, GT: 4934
Pred: <sep>, GT: held
Pred: 32001, GT: 4934
Pred: <sep>, GT: held
Pred: 32001, GT: 19098
Pred: <sep>, GT: organized
Pred: 32001, GT: 4934
Pred: <sep>, GT: held
Pred: 1, GT: 21130
Pred: <s>, GT: dod
Pred: 1, GT: 7952
Pred: <s>, GT: stay
Pred: 1, GT: 4772
Pred: <s>, GT: avoid
Pred: 1, GT: 7952
Pred: <s>, GT: stay
Pred: 1, GT: 269
Pred: <s>, GT: s
Pred: 1, GT: 1886
Pred: <s>, GT: ste
Pred: 1, GT: 25349
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 29889, GT: 32000
Pred: ., GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
11/05/2023 23:36:50 - INFO - __main__ -  [Validation] Step: 400, Scores: {"val_Accuracy": 0.0}| 7/7 [00:08<00:00,  1.14s/it]
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32002, GT: 32000
Pred: <Trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 310, GT: 32000
Pred: of, GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 29889, GT: 32000
Pred: ., GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 29889, GT: 32000
Pred: ., GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 29889, GT: 32000
Pred: ., GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 30488, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29892, GT: 32000
Pred: ,, GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 29899, GT: 32000
Pred: -, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 29871, GT: 32000
Pred: , GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32000, GT: 32000
Pred: <trigger>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
Pred: 1, GT: 32000
Pred: <s>, GT: <trigger>
Pred: 32001, GT: 32000
Pred: <sep>, GT: <trigger>
  0%|                                                                                                                                                         | 401/170984 [22:36<427:42:16,  9.03s/it]11/05/2023 23:37:04 - INFO - __main__ -   Step: 401, LR: 1.5635052149332294e-06, Loss: 0.21368157863616943
  0%|                                                                                                                                                         | 401/170984 [22:36<427:42:16,  9.03s/it]11/05/2023 23:37:04 - INFO - __main__ -   Step: 401, LR: 1.5635052149332294e-06, Loss: 0.21368157863616943
  0%|                                                                                                                                                         | 401/170984 [22:36<427:42:16,  9.03s/it]11/05/2023 23:37:04 - INFO - __main__ -   Step: 401, LR: 1.5635052149332294e-06, Loss: 0.21368157863616943
[2023-11-05 23:37:07,805] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:37:11,140] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 401/170984 [22:36<427:42:16,  9.03s/it]11/05/2023 23:37:04 - INFO - __main__ -   Step: 401, LR: 1.5635052149332294e-06, Loss: 0.21368157863616943
[2023-11-05 23:37:14,468] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 401/170984 [22:36<427:42:16,  9.03s/it]11/05/2023 23:37:04 - INFO - __main__ -   Step: 401, LR: 1.5635052149332294e-06, Loss: 0.21368157863616943
  0%|                                                                                                                                                         | 405/170984 [22:50<224:59:55,  4.75s/it]11/05/2023 23:37:17 - INFO - __main__ -   Step: 405, LR: 1.579101276927576e-06, Loss: 0.00011091094347648323
  0%|                                                                                                                                                         | 405/170984 [22:50<224:59:55,  4.75s/it]11/05/2023 23:37:17 - INFO - __main__ -   Step: 405, LR: 1.579101276927576e-06, Loss: 0.00011091094347648323
  0%|                                                                                                                                                         | 405/170984 [22:50<224:59:55,  4.75s/it]11/05/2023 23:37:17 - INFO - __main__ -   Step: 405, LR: 1.579101276927576e-06, Loss: 0.00011091094347648323
[2023-11-05 23:37:21,267] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:37:24,598] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 405/170984 [22:50<224:59:55,  4.75s/it]11/05/2023 23:37:17 - INFO - __main__ -   Step: 405, LR: 1.579101276927576e-06, Loss: 0.00011091094347648323
  0%|                                                                                                                                                         | 405/170984 [22:50<224:59:55,  4.75s/it]11/05/2023 23:37:17 - INFO - __main__ -   Step: 405, LR: 1.579101276927576e-06, Loss: 0.00011091094347648323
  0%|                                                                                                                                                         | 409/170984 [23:03<173:58:48,  3.67s/it]11/05/2023 23:37:31 - INFO - __main__ -   Step: 409, LR: 1.5946973389219223e-06, Loss: 0.0188447106629610063
  0%|                                                                                                                                                         | 409/170984 [23:03<173:58:48,  3.67s/it]11/05/2023 23:37:31 - INFO - __main__ -   Step: 409, LR: 1.5946973389219223e-06, Loss: 0.0188447106629610063
[2023-11-05 23:37:31,263] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:37:34,568] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 409/170984 [23:03<173:58:48,  3.67s/it]11/05/2023 23:37:31 - INFO - __main__ -   Step: 409, LR: 1.5946973389219223e-06, Loss: 0.0188447106629610063
  0%|                                                                                                                                                         | 409/170984 [23:03<173:58:48,  3.67s/it]11/05/2023 23:37:31 - INFO - __main__ -   Step: 409, LR: 1.5946973389219223e-06, Loss: 0.0188447106629610063
[2023-11-05 23:37:37,836] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:37:41,110] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 409/170984 [23:03<173:58:48,  3.67s/it]11/05/2023 23:37:31 - INFO - __main__ -   Step: 409, LR: 1.5946973389219223e-06, Loss: 0.0188447106629610063
  0%|                                                                                                                                                         | 413/170984 [23:16<160:19:16,  3.38s/it]11/05/2023 23:37:44 - INFO - __main__ -   Step: 413, LR: 1.6102934009162688e-06, Loss: 0.1611649245023727463
  0%|                                                                                                                                                         | 413/170984 [23:16<160:19:16,  3.38s/it]11/05/2023 23:37:44 - INFO - __main__ -   Step: 413, LR: 1.6102934009162688e-06, Loss: 0.1611649245023727463
  0%|                                                                                                                                                         | 413/170984 [23:16<160:19:16,  3.38s/it]11/05/2023 23:37:44 - INFO - __main__ -   Step: 413, LR: 1.6102934009162688e-06, Loss: 0.1611649245023727463
[2023-11-05 23:37:47,755] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:37:51,123] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 413/170984 [23:16<160:19:16,  3.38s/it]11/05/2023 23:37:44 - INFO - __main__ -   Step: 413, LR: 1.6102934009162688e-06, Loss: 0.1611649245023727463
[2023-11-05 23:37:54,525] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 417/170984 [23:29<159:01:37,  3.36s/it]11/05/2023 23:37:57 - INFO - __main__ -   Step: 417, LR: 1.6258894629106153e-06, Loss: 0.2193575054407119863
  0%|                                                                                                                                                         | 417/170984 [23:29<159:01:37,  3.36s/it]11/05/2023 23:37:57 - INFO - __main__ -   Step: 417, LR: 1.6258894629106153e-06, Loss: 0.2193575054407119863
[2023-11-05 23:37:57,831] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:38:01,150] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 417/170984 [23:29<159:01:37,  3.36s/it]11/05/2023 23:37:57 - INFO - __main__ -   Step: 417, LR: 1.6258894629106153e-06, Loss: 0.2193575054407119863
[2023-11-05 23:38:04,553] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 417/170984 [23:29<159:01:37,  3.36s/it]11/05/2023 23:37:57 - INFO - __main__ -   Step: 417, LR: 1.6258894629106153e-06, Loss: 0.2193575054407119863
  0%|                                                                                                                                                         | 417/170984 [23:29<159:01:37,  3.36s/it]11/05/2023 23:37:57 - INFO - __main__ -   Step: 417, LR: 1.6258894629106153e-06, Loss: 0.2193575054407119863
[2023-11-05 23:38:07,942] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:14,683] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:18,028] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:21,354] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:38:24,637] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:27,938] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:38:31,351] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:34,684] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:37,980] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:38:41,270] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:44,573] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:47,929] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:38:51,243] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:54,586] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:38:57,913] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:01,166] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:04,424] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:07,784] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:11,090] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:14,407] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:17,764] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:21,074] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:24,363] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:27,679] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:31,007] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:34,377] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:37,765] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:41,127] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:44,458] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:47,754] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:39:50,963] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:54,273] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:39:57,567] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:00,855] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:04,279] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:07,603] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:10,997] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:14,274] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:17,604] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:20,961] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:24,294] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:27,603] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:30,939] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:34,303] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:37,635] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:40,968] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:44,274] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:47,598] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:50,957] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:40:54,273] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:40:57,536] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:41:00,895] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:41:04,357] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:41:07,619] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:41:10,985] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:41:14,327] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[2023-11-05 23:41:17,644] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
[2023-11-05 23:41:20,988] [WARNING] [stage3.py:1933:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  0%|                                                                                                                                                         | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1903, in backwardd                                     | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093
  File "/home/caizf/miniconda3/envs/tulu/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1903, in backwardd                                     | 421/170984 [23:43<158:37:52,  3.35s/it]11/05/2023 23:38:11 - INFO - __main__ -   Step: 421, LR: 1.6414855249049616e-06, Loss: 0.0090577658265829093