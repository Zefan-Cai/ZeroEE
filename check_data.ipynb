{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /local1/ponienkung/miniconda3/envs/tulu did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/local1/ponienkung/miniconda3/envs/tulu/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-09 17:28:23,968] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "198\n",
      "0 3908 39080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoModelForCausalLM,AutoTokenizer, LlamaTokenizer, LlamaTokenizerFast\n",
    "import json\n",
    "model_name_or_path = \"/local1/zefan/output/DefScaleNewGenDatav2_200_8definition/epoch_2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if isinstance(tokenizer, LlamaTokenizer) or isinstance(tokenizer, LlamaTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens in [0, 1], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n",
    "\n",
    "\n",
    "cnt_error = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "seq_len = []\n",
    "with open(\"../data/generated_data/train_400_10definitions_v2.json\", \"r\") as F:\n",
    "    for line in F.readlines():\n",
    "        dat = json.loads(line)\n",
    "        sentence = dat['prompt'].split(\"SENTENCE:\")[1].split(\"EVENT TYPE\")[0].strip()\n",
    "        trigger = dat['completion'].replace(\"Event trigger is \", \"\").replace(\".\", \"\").strip()\n",
    "        if trigger != \"<trigger>\":\n",
    "            pos +=1\n",
    "            if not trigger in sentence:\n",
    "                cnt_error += 1\n",
    "            seq_len.append(len(tokenizer(dat[\"prompt\"] + dat[\"completion\"]).input_ids))   \n",
    "            if seq_len[-1] > 240:\n",
    "                print(dat[\"prompt\"])\n",
    "        else:\n",
    "            neg +=1\n",
    "        \n",
    "print(max(seq_len))\n",
    "print(cnt_error, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.35s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /local1/zefan/output/DefScaleNewGenDatav2_200_8definition/epoch_2 and are newly initialized: ['model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   317,  3919,  1430,  4741, 29901, 16704,   423,   674,  9055,\n",
      "           278,  8225,   310,  4832,  7602,   919, 29879,  3704,   385,  7560,\n",
      "           373,  4892,  1948,  1156,  5320,   310,   963,  5929,  7943,   304,\n",
      "           278, 22569,  9245,   363,   263,  1473,  9076,  1919,   410,  3471,\n",
      "         29560,  1497,   498,  1295,  3250,   869, 29871,    13,   382, 29963,\n",
      "          3919,   323,  6959, 29901,  6345, 29889, 29871,    13,  5012, 29943,\n",
      "          1177, 22122, 29901, 20700, 29899, 29933,  1955, 29940,  6864, 10008,\n",
      "         10940,   263,   349,  1001,  3094, 14945,   338,  2183, 12060,   304,\n",
      "         29889,  3529,  4443,   393,   591,   437,   451,  3160,   278, 12060,\n",
      "           310,   916,  2712,   470,  7014, 29889, 29871,    13,   349,  1718,\n",
      "          3919, 29901, 15426, 29892,   317,  1164, 29901, 22564, 29899, 29926,\n",
      "           737, 29892,  6507, 29899,   862,  1772, 29892, 14260, 29899,   354,\n",
      "          4362, 29892,  8323, 29899,   513,   919, 29892, 12991, 29892,  7602,\n",
      "           919, 29892, 10541, 29892,  2691, 29892,  6222, 29892, 17541,   328,\n",
      "           568, 29892, 10695,   277, 29892,   282, 20342, 29892, 25530, 29889,\n",
      "         29871,    13,  1105,   825,   338,   278,  7135, 29973,  6864,  7135,\n",
      "           338]], device='cuda:0')\n",
      "tensor([[    1,   317,  3919,  1430,  4741, 29901, 16704,   423,   674,  9055,\n",
      "           278,  8225,   310,  4832,  7602,   919, 29879,  3704,   385,  7560,\n",
      "           373,  4892,  1948,  1156,  5320,   310,   963,  5929,  7943,   304,\n",
      "           278, 22569,  9245,   363,   263,  1473,  9076,  1919,   410,  3471,\n",
      "         29560,  1497,   498,  1295,  3250,   869, 29871,    13,   382, 29963,\n",
      "          3919,   323,  6959, 29901,  6345, 29889, 29871,    13,  5012, 29943,\n",
      "          1177, 22122, 29901, 20700, 29899, 29933,  1955, 29940,  6864, 10008,\n",
      "         10940,   263,   349,  1001,  3094, 14945,   338,  2183, 12060,   304,\n",
      "         29889,  3529,  4443,   393,   591,   437,   451,  3160,   278, 12060,\n",
      "           310,   916,  2712,   470,  7014, 29889, 29871,    13,   349,  1718,\n",
      "          3919, 29901, 15426, 29892,   317,  1164, 29901, 22564, 29899, 29926,\n",
      "           737, 29892,  6507, 29899,   862,  1772, 29892, 14260, 29899,   354,\n",
      "          4362, 29892,  8323, 29899,   513,   919, 29892, 12991, 29892,  7602,\n",
      "           919, 29892, 10541, 29892,  2691, 29892,  6222, 29892, 17541,   328,\n",
      "           568, 29892, 10695,   277, 29892,   282, 20342, 29892, 25530, 29889,\n",
      "         29871,    13,  1105,   825,   338,   278,  7135, 29973,  6864,  7135,\n",
      "           338, 32000, 29889,     2]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> SENTENCE: Indonesia will delay the execution of six convicts including an Indian on death row after five of them appealed to the Supreme Court for a second review , prosecutors said Thursday . \\n EVENT TYPE: born. \\n DEFINITION: BE-BORN Event occurs whenever a PERSON Entity is given birth to. Please note that we do not include the birth of other things or ideas. \\n PARENT: justice, SON: arrest-jail, release-parole, trial-hearing, charge-indict, sue, convict, sentence, fine, execute, extradite, acquit, pardon, appeal. \\n So what is the trigger? Event trigger is<trigger>.</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(['SENTENCE: A senior administration official said the president had decided to be cautious . \\n EVENT TYPE: divorce. \\n DEFINITION: A DIVORCE Event occurs whenever two people are officially divorced under the legal definition of divorce. We do not include separations or church annulments. \\n PARENT: life, SON: be born, marry, divorce, injure, die. \\n So what is the trigger? Event trigger is']).input_ids\n",
    "# tokenizer.decode(\n",
    "#     [32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "#          32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "#          32003,   317,  3919,  1430,  4741, 29901,   319, 16336, 17517,  6221,\n",
    "#           1497,   278,  6673,   750,  8459,   304,   367,   274,  1300,  2738,\n",
    "#            869, 29871,    13,   382, 29963,  3919,   323,  6959, 29901, 25074,\n",
    "#            346, 29889, 29871,    13,  5012, 29943,  1177, 22122, 29901,   319,\n",
    "#            360,  5667,  1955,  4741,  6864, 10008, 10940,  1023,  2305,   526,\n",
    "#          22444, 25074,  1133,  1090,   278, 11706,  5023,   310, 25074,   346,\n",
    "#          29889,  1334,   437,   451,  3160,  2903,   800,   470,  6586,  2889,\n",
    "#            352,  1860, 29889, 29871,    13,   349,  1718,  3919, 29901,  2834,\n",
    "#          29892,   317,  1164, 29901,   367,  6345, 29892, 20479, 29892, 25074,\n",
    "#            346, 29892, 10899,   545, 29892,   762, 29889, 29871,    13,  1105,\n",
    "#            825,   338,   278,  7135, 29973,  6864,  7135,   338,     0,     0,\n",
    "#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
    "# )\n",
    "# tokenizer.decode([3919])\n",
    "Sentence = \"\"\"SENTENCE: Indonesia will delay the execution of six convicts including an Indian on death row after five of them appealed to the Supreme Court for a second review , prosecutors said Thursday . \n",
    " EVENT TYPE: born. \n",
    " DEFINITION: BE-BORN Event occurs whenever a PERSON Entity is given birth to. Please note that we do not include the birth of other things or ideas. \n",
    " PARENT: justice, SON: arrest-jail, release-parole, trial-hearing, charge-indict, sue, convict, sentence, fine, execute, extradite, acquit, pardon, appeal. \n",
    " So what is the trigger? Event trigger is\"\"\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(Sentence, return_tensors=\"pt\").input_ids.to(\"cuda:0\")\n",
    "# input_ids = torch.LongTensor(input_ids).to(\"cuda:0\")\n",
    "attention_mask = input_ids != 32003\n",
    "\n",
    "# print(sum([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 1, 1, 1, 1, 1, 1, 1]), torch.sum(attention_mask))\n",
    "\n",
    "output = model.generate(\n",
    "  input_ids = input_ids,\n",
    "  attention_mask = attention_mask,\n",
    "  max_new_tokens = 10,\n",
    "  stopping_criteria = None)\n",
    "\n",
    "print(input_ids)\n",
    "print(output)\n",
    "tokenizer.decode(output[0])\n",
    "# print(tokenizer.decode(output[0])[len(Sentence):].strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n"
     ]
    }
   ],
   "source": [
    "a = None\n",
    "if a != []:\n",
    "    print(\"AAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"data/generated_data_fix_v4.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_parent_list = []\n",
    "for parent in data:\n",
    "    if len(data[parent]['events']) > 8:\n",
    "        to_remove_parent_list.append(parent)\n",
    "for parent in to_remove_parent_list:\n",
    "    del data[parent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4005 3232\n",
      "1704\n",
      "11\n",
      "194 579\n"
     ]
    }
   ],
   "source": [
    "# print(data.keys())\n",
    "ontology_size = []\n",
    "events = {}\n",
    "total_events = []\n",
    "to_remove = []\n",
    "for parent in data:\n",
    "    ontology_size.append(len(data[parent]['events']))\n",
    "    for ev in data[parent]['events']:\n",
    "        if ev in events:\n",
    "            events[ev].append([parent, ontology_size[-1]])\n",
    "        else:\n",
    "            events[ev] = [[parent, ontology_size[-1]]]\n",
    "    total_events += data[parent]['events']\n",
    "    # if ontology_size[-1] > 7:\n",
    "    #     print(parent, data[parent]['events'])\n",
    "print(len(total_events), len(set(total_events)))\n",
    "ontology_size = np.array(ontology_size)\n",
    "print(len(ontology_size))\n",
    "print(np.sum(ontology_size > 7))\n",
    "\n",
    "for ev, parents in events.items():\n",
    "    events[ev] = sorted(parents, key=lambda x: x[1], reverse=True)\n",
    "remove_parent_num = 0\n",
    "remove_child_num = 0\n",
    "for ev, parents in events.items():\n",
    "    if len(parents) > 1:\n",
    "        for to_re in parents[1:]:\n",
    "            to_remove_parent = to_re[0]\n",
    "            to_remove_events = ev\n",
    "            if len(data[to_remove_parent]['events']) <= 1: #only one event, remove parents\n",
    "                del data[to_remove_parent]\n",
    "                remove_parent_num += 1\n",
    "            else:\n",
    "                remove_child_num +=1\n",
    "                data[to_remove_parent]['events'].remove(to_remove_events)\n",
    "                if to_remove_events in data[to_remove_parent]['sons']:\n",
    "                    data[to_remove_parent]['sons'].remove(to_remove_events)\n",
    "\n",
    "\n",
    "print(remove_parent_num, remove_child_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffle\n",
    "import random\n",
    "keys = list(data.keys())\n",
    "random.shuffle(keys)\n",
    "new_data = {k: data[k] for k in keys}\n",
    "with open(\"data/generated_data_fix_v5.json\", 'w') as F:\n",
    "    json.dump(new_data, F, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tulu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
