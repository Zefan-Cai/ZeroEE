{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "0 3908 39080\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoModelForCausalLM,AutoTokenizer, LlamaTokenizer, LlamaTokenizerFast\n",
    "import json\n",
    "model_name_or_path = \"/local1/zefan/output/NewGenDatav2_400_10definition/epoch_7\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if isinstance(tokenizer, LlamaTokenizer) or isinstance(tokenizer, LlamaTokenizerFast):\n",
    "    num_added_tokens = tokenizer.add_special_tokens({\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<pad>\",\n",
    "    })\n",
    "    assert num_added_tokens in [0, 1], \"LlamaTokenizer should only add one special token - the pad_token, or no tokens if pad token present.\"\n",
    "\n",
    "\n",
    "cnt_error = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "seq_len = []\n",
    "with open(\"../data/generated_data/train_400_10definitions_v2.json\", \"r\") as F:\n",
    "    for line in F.readlines():\n",
    "        dat = json.loads(line)\n",
    "        sentence = dat['prompt'].split(\"SENTENCE:\")[1].split(\"EVENT TYPE\")[0].strip()\n",
    "        trigger = dat['completion'].replace(\"Event trigger is \", \"\").replace(\".\", \"\").strip()\n",
    "        if trigger != \"<trigger>\":\n",
    "            pos +=1\n",
    "            if not trigger in sentence:\n",
    "                cnt_error += 1\n",
    "            seq_len.append(len(tokenizer(dat[\"prompt\"] + dat[\"completion\"]).input_ids))   \n",
    "            if seq_len[-1] > 240:\n",
    "                print(dat[\"prompt\"])\n",
    "        else:\n",
    "            neg +=1\n",
    "        \n",
    "print(max(seq_len))\n",
    "print(cnt_error, pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.71s/it]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /local1/zefan/output/NewGenDatav2_400_10definition/epoch_7 and are newly initialized: ['model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 tensor(164, device='cuda:0')\n",
      "tensor([[32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
      "         32003, 32003, 32003, 32003, 32003, 32003,     1,   317,  3919,  1430,\n",
      "          4741, 29901, 16704,   423,   674,  9055,   278,  8225,   310,  4832,\n",
      "          7602,   919, 29879,  3704,   385,  7560,   373,  4892,  1948,  1156,\n",
      "          5320,   310,   963,  5929,  7943,   304,   278, 22569,  9245,   363,\n",
      "           263,  1473,  9076,  1919,   410,  3471, 29560,  1497,   498,  1295,\n",
      "          3250,   869, 29871,    13,   382, 29963,  3919,   323,  6959, 29901,\n",
      "          6782,  6909, 29889, 29871,    13,  5012, 29943,  1177, 22122, 29901,\n",
      "         10014,  2190, 20322,  1001, 29899, 29924, 12413, 29979, 28488,  2737,\n",
      "           304,   278,  6820, 29892, 13442, 29892, 27942,   292, 29892,   470,\n",
      "           301,  2548,  6909,   746,   372,   338,   451,   297,   278,  3030,\n",
      "           310, 10596,  5832,  1554, 29889,   450, 24420,  6455,   526, 29901,\n",
      "           313, 29896, 29897,  2305,  6820,  6909,   304, 25700,   313,   392,\n",
      "          2805,  3078, 18806,  1821,   297,   736,   416,   322,   313, 29906,\n",
      "         29897, 25700,   301,  2548,  6909,   304,  2305,   470,   916,  1638,\n",
      "         29879, 29889, 29871,    13,   349,  1718,  3919, 29901, 10804, 29892,\n",
      "           317,  1164, 29901,  6782, 27428, 29892,  6782,  6909, 29889, 29871,\n",
      "            13,  1105,   825,   338,   278,  7135, 29973,  6864,  7135,   338,\n",
      "         32000, 29889,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# tokenizer(['SENTENCE: A senior administration official said the president had decided to be cautious . \\n EVENT TYPE: divorce. \\n DEFINITION: A DIVORCE Event occurs whenever two people are officially divorced under the legal definition of divorce. We do not include separations or church annulments. \\n PARENT: life, SON: be born, marry, divorce, injure, die. \\n So what is the trigger? Event trigger is']).input_ids\n",
    "# tokenizer.decode(\n",
    "#     [32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "#          32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "#          32003,   317,  3919,  1430,  4741, 29901,   319, 16336, 17517,  6221,\n",
    "#           1497,   278,  6673,   750,  8459,   304,   367,   274,  1300,  2738,\n",
    "#            869, 29871,    13,   382, 29963,  3919,   323,  6959, 29901, 25074,\n",
    "#            346, 29889, 29871,    13,  5012, 29943,  1177, 22122, 29901,   319,\n",
    "#            360,  5667,  1955,  4741,  6864, 10008, 10940,  1023,  2305,   526,\n",
    "#          22444, 25074,  1133,  1090,   278, 11706,  5023,   310, 25074,   346,\n",
    "#          29889,  1334,   437,   451,  3160,  2903,   800,   470,  6586,  2889,\n",
    "#            352,  1860, 29889, 29871,    13,   349,  1718,  3919, 29901,  2834,\n",
    "#          29892,   317,  1164, 29901,   367,  6345, 29892, 20479, 29892, 25074,\n",
    "#            346, 29892, 10899,   545, 29892,   762, 29889, 29871,    13,  1105,\n",
    "#            825,   338,   278,  7135, 29973,  6864,  7135,   338,     0,     0,\n",
    "#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
    "# )\n",
    "# tokenizer.decode([3919])\n",
    "\n",
    "input_ids = [[32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003, 32003,\n",
    "         32003, 32003, 32003, 32003, 32003, 32003,     1,   317,  3919,  1430,\n",
    "          4741, 29901, 16704,   423,   674,  9055,   278,  8225,   310,  4832,\n",
    "          7602,   919, 29879,  3704,   385,  7560,   373,  4892,  1948,  1156,\n",
    "          5320,   310,   963,  5929,  7943,   304,   278, 22569,  9245,   363,\n",
    "           263,  1473,  9076,  1919,   410,  3471, 29560,  1497,   498,  1295,\n",
    "          3250,   869, 29871,    13,   382, 29963,  3919,   323,  6959, 29901,\n",
    "          6782,  6909, 29889, 29871,    13,  5012, 29943,  1177, 22122, 29901,\n",
    "         10014,  2190, 20322,  1001, 29899, 29924, 12413, 29979, 28488,  2737,\n",
    "           304,   278,  6820, 29892, 13442, 29892, 27942,   292, 29892,   470,\n",
    "           301,  2548,  6909,   746,   372,   338,   451,   297,   278,  3030,\n",
    "           310, 10596,  5832,  1554, 29889,   450, 24420,  6455,   526, 29901,\n",
    "           313, 29896, 29897,  2305,  6820,  6909,   304, 25700,   313,   392,\n",
    "          2805,  3078, 18806,  1821,   297,   736,   416,   322,   313, 29906,\n",
    "         29897, 25700,   301,  2548,  6909,   304,  2305,   470,   916,  1638,\n",
    "         29879, 29889, 29871,    13,   349,  1718,  3919, 29901, 10804, 29892,\n",
    "           317,  1164, 29901,  6782, 27428, 29892,  6782,  6909, 29889, 29871,\n",
    "            13,  1105,   825,   338,   278,  7135, 29973,  6864,  7135,   338]]\n",
    "input_ids = torch.LongTensor(input_ids).to(\"cuda:0\")\n",
    "attention_mask = input_ids != 32003\n",
    "\n",
    "print(sum([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1]), torch.sum(attention_mask))\n",
    "\n",
    "output = model.generate(\n",
    "  input_ids = input_ids,\n",
    "  attention_mask = attention_mask,\n",
    "  max_new_tokens = 10,\n",
    "  stopping_criteria = None)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAA\n"
     ]
    }
   ],
   "source": [
    "a = None\n",
    "if a != []:\n",
    "    print(\"AAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"data/generated_data_fix_v4.json\", 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_parent_list = []\n",
    "for parent in data:\n",
    "    if len(data[parent]['events']) > 8:\n",
    "        to_remove_parent_list.append(parent)\n",
    "for parent in to_remove_parent_list:\n",
    "    del data[parent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4005 3232\n",
      "1704\n",
      "11\n",
      "194 579\n"
     ]
    }
   ],
   "source": [
    "# print(data.keys())\n",
    "ontology_size = []\n",
    "events = {}\n",
    "total_events = []\n",
    "to_remove = []\n",
    "for parent in data:\n",
    "    ontology_size.append(len(data[parent]['events']))\n",
    "    for ev in data[parent]['events']:\n",
    "        if ev in events:\n",
    "            events[ev].append([parent, ontology_size[-1]])\n",
    "        else:\n",
    "            events[ev] = [[parent, ontology_size[-1]]]\n",
    "    total_events += data[parent]['events']\n",
    "    # if ontology_size[-1] > 7:\n",
    "    #     print(parent, data[parent]['events'])\n",
    "print(len(total_events), len(set(total_events)))\n",
    "ontology_size = np.array(ontology_size)\n",
    "print(len(ontology_size))\n",
    "print(np.sum(ontology_size > 7))\n",
    "\n",
    "for ev, parents in events.items():\n",
    "    events[ev] = sorted(parents, key=lambda x: x[1], reverse=True)\n",
    "remove_parent_num = 0\n",
    "remove_child_num = 0\n",
    "for ev, parents in events.items():\n",
    "    if len(parents) > 1:\n",
    "        for to_re in parents[1:]:\n",
    "            to_remove_parent = to_re[0]\n",
    "            to_remove_events = ev\n",
    "            if len(data[to_remove_parent]['events']) <= 1: #only one event, remove parents\n",
    "                del data[to_remove_parent]\n",
    "                remove_parent_num += 1\n",
    "            else:\n",
    "                remove_child_num +=1\n",
    "                data[to_remove_parent]['events'].remove(to_remove_events)\n",
    "                if to_remove_events in data[to_remove_parent]['sons']:\n",
    "                    data[to_remove_parent]['sons'].remove(to_remove_events)\n",
    "\n",
    "\n",
    "print(remove_parent_num, remove_child_num)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffle\n",
    "import random\n",
    "keys = list(data.keys())\n",
    "random.shuffle(keys)\n",
    "new_data = {k: data[k] for k in keys}\n",
    "with open(\"data/generated_data_fix_v5.json\", 'w') as F:\n",
    "    json.dump(new_data, F, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tulu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
